{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# PHASE 1: COMPACT LSTM MODEL ARCHITECTURE (~15-20K PARAMETERS)\n",
    "# =============================================================\n",
    "# Architecture:\n",
    "# - Compact branches: 2 conv layers + pooling (no residual blocks)\n",
    "# - Raw EMG: extracts BOTH time and frequency domain features\n",
    "# - RMS/LMS: extracts ONLY time domain features\n",
    "# - Wiener TD: extracts ONLY time domain features\n",
    "# - Wiener FFT: extracts ONLY frequency domain features\n",
    "# - Two-layer unidirectional LSTM with increased hidden size\n",
    "# - Simple mean pooling instead of attention\n",
    "# - Strong regularization (dropout=0.8, weight_decay=0.075)\n",
    "# - Session-based data split (all users in training)\n",
    "# =============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class UltraCompactBranch(nn.Module):\n",
    "    \"\"\"Ultra-compact branch: 2 conv layers + pooling (~300 params with d_model=8, ~500 with d_model=16)\"\"\"\n",
    "    def __init__(self, in_channels=1, d_model=16):\n",
    "        super().__init__()\n",
    "        # First conv: extract basic features\n",
    "        self.conv1 = nn.Conv1d(in_channels, 8, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        # Second conv: refine features\n",
    "        self.conv2 = nn.Conv1d(8, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Global pooling + projection\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.proj = nn.Linear(16, d_model)\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout after projection\n",
    "\n",
    "    def forward(self, x):  # (batch, seq_len, channels, length) or (batch, channels, length)\n",
    "        if x.dim() == 4:\n",
    "            batch_size, seq_len, channels, length = x.shape\n",
    "            x = x.view(batch_size * seq_len, channels, length)\n",
    "            reshape = True\n",
    "        else:\n",
    "            reshape = False\n",
    "            batch_size = x.shape[0]\n",
    "            seq_len = 1\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # (batch, 8, length/2)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # (batch, 16, length/4)\n",
    "        x = self.pool(x)  # (batch, 16, 1)\n",
    "        x = x.squeeze(-1)  # (batch, 16)\n",
    "        x = self.proj(x)  # (batch, d_model)\n",
    "        x = self.dropout(x)  # Regularization\n",
    "\n",
    "        if reshape:\n",
    "            x = x.view(batch_size, seq_len, -1)  # (batch, seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "class RawEMGTimeBranch(UltraCompactBranch):\n",
    "    \"\"\"Raw EMG time-domain branch - ultra-compact version\"\"\"\n",
    "    def __init__(self, d_model=16):\n",
    "        super().__init__(in_channels=1, d_model=d_model)\n",
    "\n",
    "class RawEMGFreqBranch(nn.Module):\n",
    "    \"\"\"Raw EMG frequency-domain branch - ultra-compact version\"\"\"\n",
    "    def __init__(self, d_model=16, fft_bins=64):\n",
    "        super().__init__()\n",
    "        self.fft_bins = fft_bins\n",
    "        # Process FFT bins directly with compact conv\n",
    "        self.conv1 = nn.Conv1d(1, 8, kernel_size=5, stride=2, padding=2)  # 64 -> 32\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.conv2 = nn.Conv1d(8, 16, kernel_size=3, stride=2, padding=1)  # 32 -> 16\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.proj = nn.Linear(16, d_model)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):  # (batch, seq_len, 1, 100) - raw EMG time domain\n",
    "        if x.dim() == 4:\n",
    "            batch_size, seq_len, channels, length = x.shape\n",
    "            x = x.view(batch_size * seq_len, channels, length)\n",
    "            reshape = True\n",
    "        else:\n",
    "            reshape = False\n",
    "            batch_size = x.shape[0]\n",
    "            seq_len = 1\n",
    "            x = x.unsqueeze(0) if x.dim() == 2 else x\n",
    "\n",
    "        # Compute FFT\n",
    "        x_signal = x.squeeze(1)  # (batch, 100)\n",
    "        x_fft = torch.fft.rfft(x_signal, dim=1)  # (batch, 51)\n",
    "        x_fft_mag = torch.abs(x_fft)  # (batch, 51)\n",
    "\n",
    "        # Interpolate to 64 bins\n",
    "        if x_fft_mag.shape[1] != self.fft_bins:\n",
    "            x_fft_mag = x_fft_mag.unsqueeze(1)\n",
    "            x_fft_mag = torch.nn.functional.interpolate(\n",
    "                x_fft_mag, size=self.fft_bins, mode='linear', align_corners=False\n",
    "            )\n",
    "            x_fft_mag = x_fft_mag.squeeze(1)\n",
    "\n",
    "        x_fft_mag = x_fft_mag.unsqueeze(1)  # (batch, 1, 64)\n",
    "\n",
    "        # Process with compact conv\n",
    "        x = self.relu(self.bn1(self.conv1(x_fft_mag)))  # (batch, 8, 32)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # (batch, 16, 16)\n",
    "        x = self.pool(x)  # (batch, 16, 1)\n",
    "        x = x.squeeze(-1)  # (batch, 16)\n",
    "        x = self.proj(x)  # (batch, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if reshape:\n",
    "            x = x.view(batch_size, seq_len, -1)\n",
    "        return x\n",
    "\n",
    "class RMSLMSBranch(UltraCompactBranch):\n",
    "    \"\"\"RMS+LMS filtered EMG branch - ultra-compact version\"\"\"\n",
    "    def __init__(self, d_model=16):\n",
    "        super().__init__(in_channels=1, d_model=d_model)\n",
    "\n",
    "class WienerTDBranch(UltraCompactBranch):\n",
    "    \"\"\"Wiener filter time-domain branch - ultra-compact version\"\"\"\n",
    "    def __init__(self, d_model=16):\n",
    "        super().__init__(in_channels=1, d_model=d_model)\n",
    "\n",
    "class WienerFFTBranch(nn.Module):\n",
    "    \"\"\"Wiener FFT frequency-domain branch - ultra-compact version\"\"\"\n",
    "    def __init__(self, d_model=16, fft_bins=64):\n",
    "        super().__init__()\n",
    "        # Process pre-computed FFT bins directly\n",
    "        self.conv1 = nn.Conv1d(1, 8, kernel_size=5, stride=2, padding=2)  # 64 -> 32\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.conv2 = nn.Conv1d(8, 16, kernel_size=3, stride=2, padding=1)  # 32 -> 16\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.proj = nn.Linear(16, d_model)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):  # (batch, seq_len, 64) or (batch, 64) - FFT bins from CSV\n",
    "        if x.dim() == 3:\n",
    "            batch_size, seq_len, features = x.shape\n",
    "            x = x.view(batch_size * seq_len, 1, features)\n",
    "            reshape = True\n",
    "        else:\n",
    "            reshape = False\n",
    "            batch_size = x.shape[0]\n",
    "            seq_len = 1\n",
    "            x = x.unsqueeze(1)  # (batch, 1, 64)\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # (batch, 8, 32)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # (batch, 16, 16)\n",
    "        x = self.pool(x)  # (batch, 16, 1)\n",
    "        x = x.squeeze(-1)  # (batch, 16)\n",
    "        x = self.proj(x)  # (batch, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if reshape:\n",
    "            x = x.view(batch_size, seq_len, -1)\n",
    "        return x\n",
    "\n",
    "class IMUBranch(UltraCompactBranch):\n",
    "    \"\"\"IMU branch - ultra-compact version\"\"\"\n",
    "    def __init__(self, d_model=16):\n",
    "        super().__init__(in_channels=6, d_model=d_model)\n",
    "\n",
    "class EMGLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Phase 1: Compact multi-branch LSTM model for EMG gesture classification (~15-20K parameters)\n",
    "\n",
    "    Architecture:\n",
    "    - Compact branches: 2 conv layers + pooling (no residual blocks)\n",
    "    - Raw EMG: extracts BOTH time and frequency domain features (noisy)\n",
    "    - RMS/LMS: extracts ONLY time domain features (filtered)\n",
    "    - Wiener TD: extracts ONLY time domain features (filtered)\n",
    "    - Wiener FFT: extracts ONLY frequency domain features (filtered)\n",
    "    - Two-layer unidirectional LSTM with increased hidden size\n",
    "    - Simple mean pooling instead of attention\n",
    "    - Strong regularization (dropout=0.8)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, d_model=16, hidden_size=32, num_layers=2,\n",
    "                 dropout=0.8, sequence_length=8, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Feature extraction branches (ultra-compact)\n",
    "        # Raw EMG branches (kept for reference / potential diagnostics, not used as direct inputs)\n",
    "        # These branches learn representations of the noisy EMG signal but are not concatenated\n",
    "        # into the main feature vector. The LSTM instead relies on the filtered branches (RMS/LMS,\n",
    "        # Wiener TD/FFT) and IMU to distinguish REST vs FIST.\n",
    "        self.raw_time_branch = RawEMGTimeBranch(d_model=d_model)\n",
    "        self.raw_freq_branch = RawEMGFreqBranch(d_model=d_model, fft_bins=64)\n",
    "\n",
    "        # RMS/LMS: time domain only (filtered envelope)\n",
    "        self.rms_lms_branch = RMSLMSBranch(d_model=d_model)\n",
    "\n",
    "        # Wiener: time domain only (motion-artifact reduced)\n",
    "        self.wiener_td_branch = WienerTDBranch(d_model=d_model)\n",
    "\n",
    "        # Wiener: frequency domain only (motion-artifact reduced FFT bins)\n",
    "        self.wiener_fft_branch = WienerFFTBranch(d_model=d_model, fft_bins=64)\n",
    "\n",
    "        # IMU branch\n",
    "        self.imu_branch = IMUBranch(d_model=d_model)\n",
    "\n",
    "        # Concatenate filtered features only: rms_lms + wiener_td + wiener_fft + imu\n",
    "        # Total: 4 branches * d_model = 4 * 16 = 64 features per window\n",
    "        feature_dim = 4 * d_model\n",
    "\n",
    "        # Two-layer unidirectional LSTM (increased capacity)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=0.2 if num_layers > 1 else 0,  # Dropout between LSTM layers\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # LSTM output dimension\n",
    "        lstm_output_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "        # Simple mean pooling (0 parameters) instead of attention\n",
    "        # This reduces parameters significantly while maintaining effectiveness\n",
    "\n",
    "        # Classification head with strong regularization\n",
    "        self.dropout_cls = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(lstm_output_dim, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: Dictionary containing:\n",
    "                - 'raw': (batch, seq_len, 1, window_size) - raw EMG\n",
    "                - 'rms_lms': (batch, seq_len, 1, window_size) - RMS/LMS filtered\n",
    "                - 'wiener_td': (batch, seq_len, 1, window_size) - Wiener time domain\n",
    "                - 'wiener_fft': (batch, seq_len, 64) - Wiener frequency domain\n",
    "                - 'imu': (batch, seq_len, 6, imu_samples) - IMU data\n",
    "                - 'spectral_raw': (batch, seq_len, 5) - optional spectral features\n",
    "                - 'spectral_wiener': (batch, seq_len, 5) - optional spectral features\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch, num_classes)\n",
    "        \"\"\"\n",
    "        # Extract inputs\n",
    "        raw = batch['raw']  # (batch, seq_len, 1, window_size)\n",
    "        rms_lms = batch['rms_lms']  # (batch, seq_len, 1, window_size)\n",
    "        wiener_td = batch['wiener_td']  # (batch, seq_len, 1, window_size)\n",
    "        wiener_fft = batch['wiener_fft']  # (batch, seq_len, 64)\n",
    "        imu = batch['imu']  # (batch, seq_len, 6, imu_samples)\n",
    "\n",
    "        batch_size, seq_len = raw.shape[:2]\n",
    "\n",
    "        # Extract features from filtered branches\n",
    "        # Note: Raw EMG branches are not concatenated into the main feature vector to avoid\n",
    "        # confusing the classifier with noisy signals. The LSTM relies on filtered branches\n",
    "        # (RMS/LMS, Wiener TD/FFT) and IMU, which already encode motion-artifact-reduced\n",
    "        # time and frequency domain information.\n",
    "\n",
    "        # RMS/LMS: time domain only (filtered envelope)\n",
    "        features_rms_lms = self.rms_lms_branch(rms_lms)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Wiener: time domain only (filtered)\n",
    "        features_wiener_td = self.wiener_td_branch(wiener_td)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Wiener: frequency domain only (filtered FFT bins)\n",
    "        features_wiener_fft = self.wiener_fft_branch(wiener_fft)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # IMU features\n",
    "        features_imu = self.imu_branch(imu)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Concatenate filtered features only: [rms_lms, wiener_td, wiener_fft, imu]\n",
    "        combined_features = torch.cat([\n",
    "            features_rms_lms,       # Filtered time domain (RMS/LMS)\n",
    "            features_wiener_td,     # Filtered time domain (Wiener)\n",
    "            features_wiener_fft,    # Filtered frequency domain (Wiener)\n",
    "            features_imu            # IMU features\n",
    "        ], dim=2)  # (batch, seq_len, 4*d_model)\n",
    "\n",
    "        # LSTM processes the sequence and learns temporal patterns\n",
    "        lstm_out, (h_n, c_n) = self.lstm(combined_features)  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Simple mean pooling (0 parameters) - more efficient than attention\n",
    "        pooled = lstm_out.mean(dim=1)  # (batch, hidden_size)\n",
    "\n",
    "        # Classification with strong regularization\n",
    "        pooled = self.dropout_cls(pooled)\n",
    "        logits = self.classifier(pooled)  # (batch, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "print(\"✓ Phase 1: Compact LSTM model architecture defined (~15-20K parameters)\")\n",
    "print(\"\\nArchitecture Summary:\")\n",
    "print(\"  - Compact branches: 2 conv layers + pooling (no residual blocks)\")\n",
    "print(\"  - Raw EMG: extracts BOTH time and frequency domain features (noisy)\")\n",
    "print(\"  - RMS/LMS: extracts ONLY time domain features (filtered)\")\n",
    "print(\"  - Wiener TD: extracts ONLY time domain features (filtered)\")\n",
    "print(\"  - Wiener FFT: extracts ONLY frequency domain features (filtered)\")\n",
    "print(\"  - Two-layer unidirectional LSTM (increased capacity: d_model=16, hidden_size=32)\")\n",
    "print(\"  - Simple mean pooling (0 parameters)\")\n",
    "print(\"  - Strong regularization (dropout=0.8, weight_decay=0.075)\")\n",
    "print(\"  - Increased model capacity to address underfitting (~15-20K parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268443f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for TPU\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    DEVICE = xm.xla_device()\n",
    "    USE_TPU = True\n",
    "    print(f\"Using TPU: {DEVICE}\")\n",
    "except:\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    USE_TPU = False\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================\n",
    "# Loads data from session folders, each containing:\n",
    "# - emg_raw.csv, emg_rms_lms.csv, emg_wiener.csv, imu.csv\n",
    "# - labels.csv with start_ms, end_ms, and gesture columns for timestamp-based labeling\n",
    "# =============================================================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    "    \"\"\"Dataset for EMG gesture classification with sequential windowing and enhanced frequency features\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, window_size: int = 100, stride: int = 25,\n",
    "                 sequence_length: int = 8, normalize: bool = True, scalers: Dict = None,\n",
    "                 use_augmentation: bool = False, augmentation_prob: float = 0.5,\n",
    "                 sampling_rate: int = 500):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Root directory containing session folders\n",
    "            window_size: Number of samples per window (100 = 200ms @ 500Hz)\n",
    "            stride: Stride between windows (25 = 50ms @ 500Hz for 50% overlap)\n",
    "            sequence_length: Number of sequential windows per sample (8 = 400ms context)\n",
    "            normalize: Whether to normalize data\n",
    "            scalers: Pre-fitted scalers for normalization (if None, fit new ones)\n",
    "            use_augmentation: Whether to apply data augmentation during training\n",
    "            augmentation_prob: Probability of applying augmentation to each sample\n",
    "            sampling_rate: Sampling rate in Hz (default 500Hz for EMG)\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.sequence_length = sequence_length\n",
    "        self.normalize = normalize\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "        # Find all session directories\n",
    "        self.sessions = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])\n",
    "        print(f\"Found {len(self.sessions)} sessions\")\n",
    "\n",
    "        # Load all data\n",
    "        self.windows = []\n",
    "        self.labels = []\n",
    "        self.scalers = scalers if scalers else {}\n",
    "\n",
    "        self._load_all_sessions()\n",
    "\n",
    "        # Create sequences from windows\n",
    "        self.sequences = []\n",
    "        self.sequence_labels = []\n",
    "        self._create_sequences()\n",
    "\n",
    "        # Fit scalers if needed\n",
    "        if normalize and not scalers:\n",
    "            self._fit_scalers()\n",
    "\n",
    "        # Normalize data\n",
    "        if normalize:\n",
    "            self._normalize_data()\n",
    "\n",
    "    def _compute_spectral_features(self, signal_window, sampling_rate=500):\n",
    "        \"\"\"Compute comprehensive spectral features from a signal window\"\"\"\n",
    "        n = len(signal_window)\n",
    "        fft_vals = fft(signal_window)\n",
    "        fft_magnitude = np.abs(fft_vals[:n//2])\n",
    "\n",
    "        # Normalize to 64 bins to match CSV FFT bins\n",
    "        if len(fft_magnitude) >= 64:\n",
    "            indices = np.linspace(0, len(fft_magnitude)-1, 64).astype(int)\n",
    "            fft_magnitude_64 = fft_magnitude[indices]\n",
    "        else:\n",
    "            fft_magnitude_64 = np.interp(np.linspace(0, len(fft_magnitude)-1, 64),\n",
    "                                         np.arange(len(fft_magnitude)), fft_magnitude)\n",
    "\n",
    "        freqs = fftfreq(n, 1/sampling_rate)[:n//2]\n",
    "\n",
    "        # Compute bandpower in different frequency bands\n",
    "        low_mask = (freqs >= 20) & (freqs <= 60)\n",
    "        bandpower_low = np.sum(fft_magnitude[low_mask]**2) if np.any(low_mask) else 0.0\n",
    "\n",
    "        mid_mask = (freqs >= 60) & (freqs <= 150)\n",
    "        bandpower_mid = np.sum(fft_magnitude[mid_mask]**2) if np.any(mid_mask) else 0.0\n",
    "\n",
    "        high_mask = (freqs >= 150) & (freqs <= 250)\n",
    "        bandpower_high = np.sum(fft_magnitude[high_mask]**2) if np.any(high_mask) else 0.0\n",
    "\n",
    "        # Spectral entropy\n",
    "        power_spectrum = fft_magnitude**2\n",
    "        power_spectrum_norm = power_spectrum / (np.sum(power_spectrum) + 1e-10)\n",
    "        spectral_entropy = -np.sum(power_spectrum_norm * np.log(power_spectrum_norm + 1e-10))\n",
    "\n",
    "        # Spectral centroid\n",
    "        if np.sum(power_spectrum) > 0:\n",
    "            spectral_centroid = np.sum(freqs * power_spectrum) / np.sum(power_spectrum)\n",
    "        else:\n",
    "            spectral_centroid = 0.0\n",
    "\n",
    "        return {\n",
    "            'fft_magnitude': fft_magnitude_64.astype(np.float32),\n",
    "            'bandpower_low': np.float32(bandpower_low),\n",
    "            'bandpower_mid': np.float32(bandpower_mid),\n",
    "            'bandpower_high': np.float32(bandpower_high),\n",
    "            'spectral_entropy': np.float32(spectral_entropy),\n",
    "            'spectral_centroid': np.float32(spectral_centroid)\n",
    "        }\n",
    "\n",
    "    def _normalize_window_per_user(self, signal_window, method='z-score'):\n",
    "        \"\"\"\n",
    "        Normalize a signal window to remove user-specific baseline and scale.\n",
    "        This helps the model learn gesture patterns, not user-specific patterns.\n",
    "\n",
    "        Args:\n",
    "            signal_window: 1D numpy array of signal values\n",
    "            method: 'z-score' (default), 'min-max', or 'robust'\n",
    "\n",
    "        Returns:\n",
    "            Normalized signal window\n",
    "        \"\"\"\n",
    "        if method == 'z-score':\n",
    "            # Standardize: (x - mean) / std\n",
    "            mean = np.mean(signal_window)\n",
    "            std = np.std(signal_window)\n",
    "            if std < 1e-8:  # Avoid division by zero\n",
    "                return signal_window - mean\n",
    "            return (signal_window - mean) / std\n",
    "        elif method == 'min-max':\n",
    "            # Scale to [0, 1]\n",
    "            min_val = np.min(signal_window)\n",
    "            max_val = np.max(signal_window)\n",
    "            if max_val - min_val < 1e-8:\n",
    "                return signal_window - min_val\n",
    "            return (signal_window - min_val) / (max_val - min_val)\n",
    "        elif method == 'robust':\n",
    "            # Use median and IQR (more robust to outliers)\n",
    "            median = np.median(signal_window)\n",
    "            q75 = np.percentile(signal_window, 75)\n",
    "            q25 = np.percentile(signal_window, 25)\n",
    "            iqr = q75 - q25\n",
    "            if iqr < 1e-8:\n",
    "                return signal_window - median\n",
    "            return (signal_window - median) / iqr\n",
    "        return signal_window\n",
    "\n",
    "    def _check_timestamp_continuity(self, timestamps, session_name, signal_name, expected_rate_hz=500):\n",
    "        \"\"\"Check for timestamp gaps (UDP packet loss detection)\"\"\"\n",
    "        if len(timestamps) < 2:\n",
    "            return\n",
    "        intervals = np.diff(timestamps)\n",
    "        expected_interval = 1000 / expected_rate_hz\n",
    "        threshold = expected_interval * 1.5\n",
    "        gaps = intervals > threshold\n",
    "        if np.any(gaps):\n",
    "            gap_count = np.sum(gaps)\n",
    "            max_gap = np.max(intervals)\n",
    "            gap_pct = 100 * gap_count / len(intervals)\n",
    "            if gap_pct > 1.0:\n",
    "                print(f\"⚠️  {session_name} {signal_name}: {gap_count} timestamp gaps ({gap_pct:.2f}%), max gap: {max_gap:.2f}ms\")\n",
    "\n",
    "    def _augment_window(self, raw_window, noise_level=0.1):\n",
    "        \"\"\"\n",
    "        Enhanced data augmentation for EMG signals\n",
    "        Carefully designed to preserve signal characteristics while increasing diversity\n",
    "        \"\"\"\n",
    "        augmented = raw_window.copy()\n",
    "\n",
    "        # 1. Gaussian noise (simulating sensor noise) - keep original\n",
    "        noise_std = np.std(raw_window) * noise_level\n",
    "        noise = np.random.normal(0, noise_std, size=raw_window.shape)\n",
    "        augmented = augmented + noise\n",
    "\n",
    "        # 2. Amplitude scaling (simulating different muscle activation levels) - keep original\n",
    "        scale_factor = np.random.uniform(0.8, 1.2)\n",
    "        augmented = augmented * scale_factor\n",
    "\n",
    "        # 3. Time warping (slight stretching/compression) - keep original\n",
    "        if len(augmented) > 10:\n",
    "            warp_factor = np.random.uniform(0.95, 1.05)\n",
    "            original_indices = np.arange(len(augmented))\n",
    "            warped_indices = original_indices * warp_factor\n",
    "            warped_indices = np.clip(warped_indices, 0, len(augmented) - 1).astype(int)\n",
    "            augmented = augmented[warped_indices]\n",
    "\n",
    "        # 4. Time shifting (small temporal delays) - NEW\n",
    "        # Shift by up to 5% of window length (preserves signal structure)\n",
    "        max_shift = max(1, int(len(augmented) * 0.05))\n",
    "        shift = np.random.randint(-max_shift, max_shift + 1)\n",
    "        if shift != 0:\n",
    "            if shift > 0:\n",
    "                # Shift right: move beginning to end\n",
    "                augmented = np.concatenate([augmented[shift:], augmented[:shift]])\n",
    "            else:\n",
    "                # Shift left: move end to beginning (shift is negative)\n",
    "                augmented = np.concatenate([augmented[shift:], augmented[:shift]])\n",
    "\n",
    "        # 5. Frequency domain augmentation (slight frequency shifts) - NEW\n",
    "        # Apply small frequency shift via phase manipulation (preserves amplitude)\n",
    "        if len(augmented) > 20:\n",
    "            # Compute FFT\n",
    "            fft_signal = np.fft.fft(augmented)\n",
    "            # Apply small random phase shift (max 5% frequency shift)\n",
    "            phase_shift = np.random.uniform(-0.05, 0.05) * 2 * np.pi\n",
    "            freqs = np.fft.fftfreq(len(augmented))\n",
    "            phase_shift_array = np.exp(1j * phase_shift * freqs)\n",
    "            fft_signal_shifted = fft_signal * phase_shift_array\n",
    "            # Convert back to time domain\n",
    "            augmented = np.real(np.fft.ifft(fft_signal_shifted))\n",
    "\n",
    "        return augmented.astype(np.float32)\n",
    "\n",
    "    def _load_all_sessions(self):\n",
    "        \"\"\"Load data from all sessions and create windows\"\"\"\n",
    "        label_encoder = LabelEncoder()\n",
    "        all_gestures = []\n",
    "\n",
    "        for session_dir in self.sessions:\n",
    "            try:\n",
    "                # Load CSV files from session folder\n",
    "                emg_raw = pd.read_csv(session_dir / 'emg_raw.csv')\n",
    "                emg_rms_lms = pd.read_csv(session_dir / 'emg_rms_lms.csv')\n",
    "                emg_wiener = pd.read_csv(session_dir / 'emg_wiener.csv')\n",
    "                imu = pd.read_csv(session_dir / 'imu.csv')\n",
    "                labels_df = pd.read_csv(session_dir / 'labels.csv')  # IMPORTANT: labels.csv with start_ms, end_ms, gesture\n",
    "\n",
    "                # Extract timestamps\n",
    "                timestamps_raw = emg_raw['timestamp_ms'].values\n",
    "                timestamps_rms = emg_rms_lms['timestamp_ms'].values\n",
    "                timestamps_wiener = emg_wiener['timestamp_ms'].values\n",
    "                timestamps_imu = imu['timestamp_ms'].values\n",
    "\n",
    "                # Check for timestamp gaps\n",
    "                self._check_timestamp_continuity(timestamps_raw, session_dir.name, 'raw_EMG', expected_rate_hz=500)\n",
    "                self._check_timestamp_continuity(timestamps_imu, session_dir.name, 'IMU', expected_rate_hz=100)\n",
    "\n",
    "                # Extract signal data\n",
    "                raw_signal = emg_raw['ch1'].values\n",
    "                rms_signal = emg_rms_lms['rms_ch1'].values\n",
    "                lms_signal = emg_rms_lms['lms_ch1'].values\n",
    "                wiener_td = emg_wiener['wiener_td_ch1'].values\n",
    "\n",
    "                # Extract FFT bins (64 bins: fft_bin_0 to fft_bin_63)\n",
    "                fft_cols = [f'fft_bin_{i}' for i in range(64)]\n",
    "                wiener_fft = emg_wiener[fft_cols].values if all(col in emg_wiener.columns for col in fft_cols) else None\n",
    "\n",
    "                # IMU data\n",
    "                imu_data = imu[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].values\n",
    "\n",
    "                # Create label mapping from timestamps using labels.csv\n",
    "                label_map = self._create_label_map(labels_df, timestamps_raw)\n",
    "\n",
    "                # Create windows\n",
    "                session_windows, session_labels = self._create_windows(\n",
    "                    timestamps_raw, raw_signal,\n",
    "                    timestamps_rms, rms_signal, lms_signal,\n",
    "                    timestamps_wiener, wiener_td, wiener_fft,\n",
    "                    timestamps_imu, imu_data,\n",
    "                    label_map\n",
    "                )\n",
    "\n",
    "                self.windows.extend(session_windows)\n",
    "                self.labels.extend(session_labels)\n",
    "                all_gestures.extend(session_labels)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading session {session_dir.name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(all_gestures)\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "        print(f\"Classes: {self.label_encoder.classes_}\")\n",
    "        print(f\"Total windows: {len(self.windows)}\")\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        \"\"\"Create sequences of sequential windows\"\"\"\n",
    "        i = 0\n",
    "        while i + self.sequence_length <= len(self.windows):\n",
    "            sequence = self.windows[i:i+self.sequence_length]\n",
    "            sequence_window_labels = self.labels[i:i+self.sequence_length]\n",
    "\n",
    "            # Use majority label for the sequence\n",
    "            unique_labels, counts = np.unique(sequence_window_labels, return_counts=True)\n",
    "            majority_label_idx = np.argmax(counts)\n",
    "            label = unique_labels[majority_label_idx]\n",
    "\n",
    "            # Only accept sequences with reasonable label consistency (>60% same label)\n",
    "            label_consistency = np.max(counts) / len(sequence_window_labels)\n",
    "            if label_consistency < 0.60:\n",
    "                i += self.stride\n",
    "                continue\n",
    "\n",
    "            self.sequences.append(sequence)\n",
    "            self.sequence_labels.append(label)\n",
    "            i += self.stride\n",
    "\n",
    "        print(f\"Total sequences: {len(self.sequences)} (each with {self.sequence_length} windows)\")\n",
    "\n",
    "    def _create_label_map(self, labels_df: pd.DataFrame, timestamps: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create label array aligned with timestamps from labels.csv\n",
    "        Uses start_ms, end_ms, and gesture columns to label the data\n",
    "        \"\"\"\n",
    "        label_map = np.full(len(timestamps), -1, dtype=int)\n",
    "\n",
    "        for _, row in labels_df.iterrows():\n",
    "            start_ms = row['start_ms']\n",
    "            end_ms = row['end_ms']\n",
    "            gesture = row['gesture']\n",
    "\n",
    "            # Skip MIXED labels - these are block-level summaries, not actual gestures\n",
    "            if gesture == 'MIXED':\n",
    "                continue\n",
    "\n",
    "            # Handle zero-duration labels (common in motion_artifact and electrode_drift blocks)\n",
    "            if start_ms == end_ms:\n",
    "                closest_idx = np.argmin(np.abs(timestamps - start_ms))\n",
    "                closest_time = timestamps[closest_idx]\n",
    "                window_half_ms = 10  # 10ms on each side = 20ms total\n",
    "                start_ms = max(timestamps[0], closest_time - window_half_ms)\n",
    "                end_ms = min(timestamps[-1], closest_time + window_half_ms)\n",
    "\n",
    "            # Find indices within this time range\n",
    "            mask = (timestamps >= start_ms) & (timestamps <= end_ms)\n",
    "\n",
    "            if not np.any(mask):\n",
    "                closest_idx = np.argmin(np.abs(timestamps - (start_ms + end_ms) / 2))\n",
    "                mask = np.zeros(len(timestamps), dtype=bool)\n",
    "                mask[closest_idx] = True\n",
    "\n",
    "            # Assign labels (don't overwrite existing labels)\n",
    "            if gesture == 'REST':\n",
    "                label_map[mask & (label_map == -1)] = 0\n",
    "            elif gesture == 'FIST':\n",
    "                label_map[mask & (label_map == -1)] = 1\n",
    "\n",
    "        return label_map\n",
    "\n",
    "    def _create_windows(self, ts_raw, raw, ts_rms, rms, lms, ts_wiener, wiener_td, wiener_fft,\n",
    "                       ts_imu, imu, label_map):\n",
    "        \"\"\"Create overlapping windows from time-series data\"\"\"\n",
    "        windows = []\n",
    "        labels = []\n",
    "\n",
    "        max_len = len(raw)\n",
    "        i = 0\n",
    "\n",
    "        while i + self.window_size <= max_len:\n",
    "            end_idx = i + self.window_size\n",
    "            window_labels = label_map[i:end_idx]\n",
    "\n",
    "            # Skip completely unlabeled windows\n",
    "            if np.all(window_labels == -1):\n",
    "                i += self.stride\n",
    "                continue\n",
    "\n",
    "            # Calculate label distribution in window\n",
    "            unique_labels, counts = np.unique(window_labels, return_counts=True)\n",
    "            labeled_mask = unique_labels != -1\n",
    "            if not np.any(labeled_mask):\n",
    "                i += self.stride\n",
    "                continue\n",
    "\n",
    "            labeled_unique = unique_labels[labeled_mask]\n",
    "            labeled_counts = counts[labeled_mask]\n",
    "\n",
    "            # Only accept windows with reasonable label purity (>70% same label)\n",
    "            label_purity = np.max(labeled_counts) / np.sum(labeled_counts)\n",
    "            if label_purity < 0.70:\n",
    "                i += self.stride\n",
    "                continue\n",
    "\n",
    "            # Use majority label\n",
    "            majority_label_idx = np.argmax(labeled_counts)\n",
    "            label = labeled_unique[majority_label_idx]\n",
    "\n",
    "            if label == -1:\n",
    "                i += self.stride\n",
    "                continue\n",
    "\n",
    "            # Extract EMG windows\n",
    "            raw_window = raw[i:end_idx].copy()\n",
    "\n",
    "            # Apply augmentation if enabled\n",
    "            if self.use_augmentation and np.random.random() < self.augmentation_prob:\n",
    "                raw_window = self._augment_window(raw_window)\n",
    "\n",
    "            # Align RMS/LMS and Wiener signals using timestamps\n",
    "            window_start_time = ts_raw[i]\n",
    "            window_end_time = ts_raw[end_idx-1]\n",
    "\n",
    "            # RMS: Downsampled envelope (~10Hz) - interpolate\n",
    "            rms_mask = (ts_rms >= window_start_time) & (ts_rms <= window_end_time)\n",
    "            rms_indices = np.where(rms_mask)[0]\n",
    "            if len(rms_indices) >= 2:\n",
    "                rms_window = np.interp(ts_raw[i:end_idx], ts_rms[rms_indices], rms[rms_indices])\n",
    "            elif len(rms_indices) == 1:\n",
    "                rms_window = np.full(self.window_size, rms[rms_indices[0]])\n",
    "            else:\n",
    "                nearest_rms_idx = np.argmin(np.abs(ts_rms - window_start_time))\n",
    "                rms_window = np.full(self.window_size, rms[nearest_rms_idx])\n",
    "\n",
    "            # LMS: Full rate (500Hz)\n",
    "            lms_mask = (ts_rms >= window_start_time) & (ts_rms <= window_end_time)\n",
    "            lms_indices = np.where(lms_mask)[0]\n",
    "            if len(lms_indices) >= self.window_size:\n",
    "                lms_window = lms[lms_indices[:self.window_size]]\n",
    "            elif len(lms_indices) >= 2:\n",
    "                lms_window = np.interp(ts_raw[i:end_idx], ts_rms[lms_indices], lms[lms_indices])\n",
    "            elif len(lms_indices) == 1:\n",
    "                lms_window = np.full(self.window_size, lms[lms_indices[0]])\n",
    "            else:\n",
    "                nearest_lms_idx = np.argmin(np.abs(ts_rms - window_start_time))\n",
    "                lms_window = np.full(self.window_size, lms[nearest_lms_idx])\n",
    "\n",
    "            # Wiener TD\n",
    "            wiener_mask = (ts_wiener >= window_start_time) & (ts_wiener <= window_end_time)\n",
    "            wiener_indices = np.where(wiener_mask)[0]\n",
    "            if len(wiener_indices) >= self.window_size:\n",
    "                wiener_td_window = wiener_td[wiener_indices[:self.window_size]]\n",
    "            elif len(wiener_indices) >= 2:\n",
    "                wiener_td_window = np.interp(ts_raw[i:end_idx], ts_wiener[wiener_indices], wiener_td[wiener_indices])\n",
    "            elif len(wiener_indices) == 1:\n",
    "                wiener_td_window = np.full(self.window_size, wiener_td[wiener_indices[0]])\n",
    "            else:\n",
    "                nearest_wiener_idx = np.argmin(np.abs(ts_wiener - window_start_time))\n",
    "                wiener_td_window = np.full(self.window_size, wiener_td[nearest_wiener_idx])\n",
    "\n",
    "            # Compute spectral features\n",
    "            spectral_features_raw = self._compute_spectral_features(raw_window, self.sampling_rate)\n",
    "            spectral_features_wiener = self._compute_spectral_features(wiener_td_window, self.sampling_rate)\n",
    "\n",
    "            # Use pre-computed Wiener FFT bins from CSV (64 bins)\n",
    "            window_center_time = (window_start_time + window_end_time) / 2\n",
    "            if wiener_fft is not None and len(wiener_fft) > 0:\n",
    "                fft_time_distances = np.abs(ts_wiener - window_center_time)\n",
    "                closest_fft_idx = np.argmin(fft_time_distances)\n",
    "                if closest_fft_idx < len(wiener_fft):\n",
    "                    wiener_fft_window = wiener_fft[closest_fft_idx]\n",
    "                else:\n",
    "                    wiener_fft_window = wiener_fft[-1]\n",
    "            else:\n",
    "                wiener_fft_window = np.zeros(64)\n",
    "\n",
    "            # Extract IMU window (100Hz)\n",
    "            window_duration_ms = (end_idx - i) / self.sampling_rate * 1000\n",
    "            imu_samples_needed = int(window_duration_ms / 10)\n",
    "            imu_mask = (ts_imu >= window_start_time) & (ts_imu <= window_end_time)\n",
    "            imu_indices = np.where(imu_mask)[0]\n",
    "            if len(imu_indices) >= imu_samples_needed:\n",
    "                imu_window = imu[imu_indices[:imu_samples_needed]]\n",
    "            elif len(imu_indices) > 0:\n",
    "                imu_window = np.zeros((imu_samples_needed, 6))\n",
    "                imu_window[:len(imu_indices)] = imu[imu_indices]\n",
    "            else:\n",
    "                nearest_idx = np.argmin(np.abs(ts_imu - window_start_time))\n",
    "                imu_window = np.tile(imu[nearest_idx], (imu_samples_needed, 1))\n",
    "\n",
    "            # =============================================================\n",
    "            # STEP 1: PER-WINDOW Z-SCORE NORMALIZATION\n",
    "            # =============================================================\n",
    "            # Normalize each window to remove user-specific baselines/scales.\n",
    "            # This is critical for LOUO - makes signals comparable across different users.\n",
    "            # This step removes user-specific amplitude differences while preserving\n",
    "            # the relative signal structure within each window.\n",
    "            # =============================================================\n",
    "            raw_window = self._normalize_window_per_user(raw_window, method='z-score')\n",
    "            rms_window = self._normalize_window_per_user(rms_window, method='z-score')\n",
    "            lms_window = self._normalize_window_per_user(lms_window, method='z-score')\n",
    "            wiener_td_window = self._normalize_window_per_user(wiener_td_window, method='z-score')\n",
    "            # Note: wiener_fft is already in frequency domain, normalize it too\n",
    "            if len(wiener_fft_window) > 0:\n",
    "                wiener_fft_window = self._normalize_window_per_user(wiener_fft_window, method='z-score')\n",
    "            # IMU: normalize each channel separately (imu_window is shape (6, samples))\n",
    "            imu_window_normalized = np.zeros_like(imu_window)\n",
    "            for ch in range(imu_window.shape[0]):  # 6 channels\n",
    "                imu_window_normalized[ch] = self._normalize_window_per_user(imu_window[ch], method='z-score')\n",
    "            imu_window = imu_window_normalized\n",
    "\n",
    "            # Pack window data\n",
    "            window_data = {\n",
    "                'raw': raw_window.astype(np.float32),\n",
    "                'rms_lms': np.stack([rms_window, lms_window], axis=0).astype(np.float32),\n",
    "                'wiener_td': wiener_td_window.astype(np.float32),\n",
    "                'wiener_fft': wiener_fft_window.astype(np.float32),\n",
    "                'spectral_features_raw': {\n",
    "                    'bandpower_low': spectral_features_raw['bandpower_low'],\n",
    "                    'bandpower_mid': spectral_features_raw['bandpower_mid'],\n",
    "                    'bandpower_high': spectral_features_raw['bandpower_high'],\n",
    "                    'spectral_entropy': spectral_features_raw['spectral_entropy'],\n",
    "                    'spectral_centroid': spectral_features_raw['spectral_centroid']\n",
    "                },\n",
    "                'spectral_features_wiener': {\n",
    "                    'bandpower_low': spectral_features_wiener['bandpower_low'],\n",
    "                    'bandpower_mid': spectral_features_wiener['bandpower_mid'],\n",
    "                    'bandpower_high': spectral_features_wiener['bandpower_high'],\n",
    "                    'spectral_entropy': spectral_features_wiener['spectral_entropy'],\n",
    "                    'spectral_centroid': spectral_features_wiener['spectral_centroid']\n",
    "                },\n",
    "                'imu': imu_window.T.astype(np.float32)\n",
    "            }\n",
    "\n",
    "            windows.append(window_data)\n",
    "            labels.append(label)\n",
    "            i += self.stride\n",
    "\n",
    "        return windows, labels\n",
    "\n",
    "    def _fit_scalers(self):\n",
    "        \"\"\"\n",
    "        Fit StandardScalers for STEP 2: GLOBAL NORMALIZATION\n",
    "        \n",
    "        After per-window z-score normalization (STEP 1), we fit global StandardScalers\n",
    "        on all training windows. This preserves REST vs FIST amplitude differences\n",
    "        across the entire dataset, which is important for discriminative learning.\n",
    "        \n",
    "        These scalers are saved and must be applied in deployment to match training normalization.\n",
    "        \"\"\"\n",
    "        raw_data = np.concatenate([w['raw'] for w in self.windows])\n",
    "        rms_lms_data = np.concatenate([w['rms_lms'].flatten() for w in self.windows])\n",
    "        wiener_td_data = np.concatenate([w['wiener_td'] for w in self.windows])\n",
    "        wiener_fft_data = np.stack([w['wiener_fft'] for w in self.windows])\n",
    "        imu_data = np.concatenate([w['imu'].flatten() for w in self.windows])\n",
    "\n",
    "        spectral_raw_features = np.array([\n",
    "            [w['spectral_features_raw']['bandpower_low'],\n",
    "             w['spectral_features_raw']['bandpower_mid'],\n",
    "             w['spectral_features_raw']['bandpower_high'],\n",
    "             w['spectral_features_raw']['spectral_entropy'],\n",
    "             w['spectral_features_raw']['spectral_centroid']]\n",
    "            for w in self.windows\n",
    "        ])\n",
    "\n",
    "        spectral_wiener_features = np.array([\n",
    "            [w['spectral_features_wiener']['bandpower_low'],\n",
    "             w['spectral_features_wiener']['bandpower_mid'],\n",
    "             w['spectral_features_wiener']['bandpower_high'],\n",
    "             w['spectral_features_wiener']['spectral_entropy'],\n",
    "             w['spectral_features_wiener']['spectral_centroid']]\n",
    "            for w in self.windows\n",
    "        ])\n",
    "\n",
    "        self.scalers['raw'] = StandardScaler().fit(raw_data.reshape(-1, 1))\n",
    "        self.scalers['rms_lms'] = StandardScaler().fit(rms_lms_data.reshape(-1, 1))\n",
    "        self.scalers['wiener_td'] = StandardScaler().fit(wiener_td_data.reshape(-1, 1))\n",
    "        self.scalers['wiener_fft'] = StandardScaler().fit(wiener_fft_data)\n",
    "        self.scalers['imu'] = StandardScaler().fit(imu_data.reshape(-1, 1))\n",
    "        self.scalers['spectral_raw'] = StandardScaler().fit(spectral_raw_features)\n",
    "        self.scalers['spectral_wiener'] = StandardScaler().fit(spectral_wiener_features)\n",
    "\n",
    "    def _normalize_data(self):\n",
    "        \"\"\"\n",
    "        STEP 2: GLOBAL STANDARDSCALER NORMALIZATION\n",
    "        \n",
    "        Apply the fitted StandardScalers to all windows. This step:\n",
    "        1. Preserves REST vs FIST amplitude differences across the dataset\n",
    "        2. Ensures consistent feature scaling for the model\n",
    "        3. Must be replicated in deployment using saved scalers\n",
    "        \n",
    "        Combined with STEP 1 (per-window z-score), this two-step normalization:\n",
    "        - Removes user-specific baselines/scales (STEP 1)\n",
    "        - Preserves class-discriminative amplitude patterns (STEP 2)\n",
    "        \"\"\"\n",
    "        for window in self.windows:\n",
    "            window['raw'] = self.scalers['raw'].transform(window['raw'].reshape(-1, 1)).flatten()\n",
    "            rms_lms_flat = window['rms_lms'].flatten()\n",
    "            rms_lms_norm = self.scalers['rms_lms'].transform(rms_lms_flat.reshape(-1, 1)).flatten()\n",
    "            window['rms_lms'] = rms_lms_norm.reshape(2, -1)\n",
    "            window['wiener_td'] = self.scalers['wiener_td'].transform(window['wiener_td'].reshape(-1, 1)).flatten()\n",
    "            window['wiener_fft'] = self.scalers['wiener_fft'].transform(window['wiener_fft'].reshape(1, -1)).flatten()\n",
    "            imu_flat = window['imu'].flatten()\n",
    "            imu_norm = self.scalers['imu'].transform(imu_flat.reshape(-1, 1)).flatten()\n",
    "            window['imu'] = imu_norm.reshape(6, -1)\n",
    "\n",
    "            spectral_raw = np.array([\n",
    "                window['spectral_features_raw']['bandpower_low'],\n",
    "                window['spectral_features_raw']['bandpower_mid'],\n",
    "                window['spectral_features_raw']['bandpower_high'],\n",
    "                window['spectral_features_raw']['spectral_entropy'],\n",
    "                window['spectral_features_raw']['spectral_centroid']\n",
    "            ]).reshape(1, -1)\n",
    "            spectral_raw_norm = self.scalers['spectral_raw'].transform(spectral_raw).flatten()\n",
    "            window['spectral_features_raw'] = {\n",
    "                'bandpower_low': spectral_raw_norm[0],\n",
    "                'bandpower_mid': spectral_raw_norm[1],\n",
    "                'bandpower_high': spectral_raw_norm[2],\n",
    "                'spectral_entropy': spectral_raw_norm[3],\n",
    "                'spectral_centroid': spectral_raw_norm[4]\n",
    "            }\n",
    "\n",
    "            spectral_wiener = np.array([\n",
    "                window['spectral_features_wiener']['bandpower_low'],\n",
    "                window['spectral_features_wiener']['bandpower_mid'],\n",
    "                window['spectral_features_wiener']['bandpower_high'],\n",
    "                window['spectral_features_wiener']['spectral_entropy'],\n",
    "                window['spectral_features_wiener']['spectral_centroid']\n",
    "            ]).reshape(1, -1)\n",
    "            spectral_wiener_norm = self.scalers['spectral_wiener'].transform(spectral_wiener).flatten()\n",
    "            window['spectral_features_wiener'] = {\n",
    "                'bandpower_low': spectral_wiener_norm[0],\n",
    "                'bandpower_mid': spectral_wiener_norm[1],\n",
    "                'bandpower_high': spectral_wiener_norm[2],\n",
    "                'spectral_entropy': spectral_wiener_norm[3],\n",
    "                'spectral_centroid': spectral_wiener_norm[4]\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.sequence_labels[idx]\n",
    "\n",
    "        # Stack windows into sequences\n",
    "        raw_seq = []\n",
    "        rms_lms_seq = []\n",
    "        wiener_td_seq = []\n",
    "        wiener_fft_seq = []\n",
    "        imu_seq = []\n",
    "        spectral_raw_seq = []\n",
    "        spectral_wiener_seq = []\n",
    "\n",
    "        for window in sequence:\n",
    "            raw_seq.append(window['raw'])\n",
    "            rms_lms_seq.append(window['rms_lms'][0])  # Using RMS only\n",
    "            wiener_td_seq.append(window['wiener_td'])\n",
    "            wiener_fft_seq.append(window['wiener_fft'])\n",
    "            imu_seq.append(window['imu'])\n",
    "\n",
    "            spectral_raw_seq.append([\n",
    "                window['spectral_features_raw']['bandpower_low'],\n",
    "                window['spectral_features_raw']['bandpower_mid'],\n",
    "                window['spectral_features_raw']['bandpower_high'],\n",
    "                window['spectral_features_raw']['spectral_entropy'],\n",
    "                window['spectral_features_raw']['spectral_centroid']\n",
    "            ])\n",
    "            spectral_wiener_seq.append([\n",
    "                window['spectral_features_wiener']['bandpower_low'],\n",
    "                window['spectral_features_wiener']['bandpower_mid'],\n",
    "                window['spectral_features_wiener']['bandpower_high'],\n",
    "                window['spectral_features_wiener']['spectral_entropy'],\n",
    "                window['spectral_features_wiener']['spectral_centroid']\n",
    "            ])\n",
    "\n",
    "        return {\n",
    "            'raw': torch.FloatTensor(np.stack(raw_seq)).unsqueeze(1),  # (seq_len, 1, window_size)\n",
    "            'rms_lms': torch.FloatTensor(np.stack(rms_lms_seq)).unsqueeze(1),  # (seq_len, 1, window_size)\n",
    "            'wiener_td': torch.FloatTensor(np.stack(wiener_td_seq)).unsqueeze(1),  # (seq_len, 1, window_size)\n",
    "            'wiener_fft': torch.FloatTensor(np.stack(wiener_fft_seq)),  # (seq_len, 64)\n",
    "            'spectral_raw': torch.FloatTensor(np.stack(spectral_raw_seq)),  # (seq_len, 5)\n",
    "            'spectral_wiener': torch.FloatTensor(np.stack(spectral_wiener_seq)),  # (seq_len, 5)\n",
    "            'imu': torch.FloatTensor(np.stack(imu_seq)),  # (seq_len, 6, imu_samples)\n",
    "            'label': torch.LongTensor([label])[0]\n",
    "        }\n",
    "\n",
    "class EMGDatasetFiltered(EMGDataset):\n",
    "    \"\"\"EMG Dataset with session filtering for train/validation/test splits\"\"\"\n",
    "    def __init__(self, data_dir: str, include_sessions: List[str] = None,\n",
    "                 window_size: int = 100, stride: int = 25, sequence_length: int = 8,\n",
    "                 normalize: bool = True, scalers: Dict = None,\n",
    "                 use_augmentation: bool = False, augmentation_prob: float = 0.5,\n",
    "                 sampling_rate: int = 500):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.sequence_length = sequence_length\n",
    "        self.normalize = normalize\n",
    "        self.include_sessions = include_sessions\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "        # Find all session directories\n",
    "        all_sessions = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "        # Filter sessions if needed\n",
    "        if self.include_sessions:\n",
    "            self.sessions = [s for s in all_sessions if s.name in self.include_sessions]\n",
    "        else:\n",
    "            self.sessions = all_sessions\n",
    "\n",
    "        print(f\"Found {len(self.sessions)} sessions (filtered)\")\n",
    "\n",
    "        # Load all data\n",
    "        self.windows = []\n",
    "        self.labels = []\n",
    "        self.scalers = scalers if scalers else {}\n",
    "\n",
    "        self._load_all_sessions()\n",
    "\n",
    "        # Create sequences from windows\n",
    "        self.sequences = []\n",
    "        self.sequence_labels = []\n",
    "        self._create_sequences()\n",
    "\n",
    "        # Fit scalers if needed\n",
    "        if normalize and not scalers:\n",
    "            self._fit_scalers()\n",
    "\n",
    "        # Normalize data\n",
    "        if normalize:\n",
    "            self._normalize_data()\n",
    "\n",
    "print(\"✓ EMGDataset and EMGDatasetFiltered classes defined\")\n",
    "print(\"  - Loads data from session folders\")\n",
    "print(\"  - Uses labels.csv with start_ms, end_ms, and gesture columns for timestamp-based labeling\")\n",
    "print(\"  - TWO-STEP NORMALIZATION ENABLED:\")\n",
    "print(\"    STEP 1: Per-window z-score normalization (removes user-specific baselines/scales)\")\n",
    "print(\"    STEP 2: Global StandardScaler normalization (preserves REST vs FIST amplitude differences)\")\n",
    "print(\"    This is critical for LOUO cross-validation and deployment consistency\")\n",
    "print(\"    StandardScalers are saved and must be applied in deployment to match training normalization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# LOSS FUNCTION: Focal Loss\n",
    "# =============================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weighting factor for each class (tensor). If None, uses uniform weights.\n",
    "            gamma: Focusing parameter. Higher gamma focuses more on hard examples.\n",
    "            reduction: 'mean' or 'sum'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if alpha is not None:\n",
    "            self.register_buffer('alpha', alpha)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate CE loss without weights first\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        # Apply alpha per-class weighting to focal loss\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]  # Get alpha for each sample's true class\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "print(\"✓ Focal Loss defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# TRAINING AND EVALUATION FUNCTIONS\n",
    "# =============================================================\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, print_per_class_loss=False):\n",
    "    \"\"\"Train for one epoch with gradient clipping and per-class loss tracking\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    per_class_loss = {}  # Track loss per class for diagnostics\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        labels = batch['label']\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Track per-class loss for diagnostics\n",
    "        if print_per_class_loss:\n",
    "            with torch.no_grad():\n",
    "                ce_loss = nn.functional.cross_entropy(logits, labels, reduction='none')\n",
    "                for cls in torch.unique(labels):\n",
    "                    cls_mask = labels == cls\n",
    "                    if cls_mask.any():\n",
    "                        if cls.item() not in per_class_loss:\n",
    "                            per_class_loss[cls.item()] = []\n",
    "                        per_class_loss[cls.item()].extend(ce_loss[cls_mask].cpu().numpy())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Print per-class loss if requested\n",
    "    if print_per_class_loss and per_class_loss:\n",
    "        print(f\"    Per-class avg loss: \", end=\"\")\n",
    "        for cls in sorted(per_class_loss.keys()):\n",
    "            avg_loss = np.mean(per_class_loss[cls])\n",
    "            print(f\"Class {cls}: {avg_loss:.4f}  \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move to device\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            labels = batch['label']\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(batch)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, cm\n",
    "\n",
    "print(\"✓ Training and evaluation functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0edb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# SINGLE USER TEST (Leave-One-User-Out Style)\n",
    "# =============================================================\n",
    "# Tests true user generalization: train on all users except userB,\n",
    "# validate to tune hyperparameters, then test on completely unseen userB.\n",
    "# This simulates \"new person walking in off the street\" without computationally\n",
    "# expensive retraining on previously seen data.\n",
    "# =============================================================\n",
    "\n",
    "# Configuration (same as Cell 4)\n",
    "SEQUENCE_LENGTH = 8  # Number of sequential windows per sample\n",
    "WINDOW_SIZE = 100    # Window size in samples (200ms @ 500Hz)\n",
    "STRIDE = 25          # Stride between windows (50ms @ 500Hz, 50% overlap)\n",
    "\n",
    "# PHASE 1: Compact LSTM Model hyperparameters with increased capacity\n",
    "# Increased from d_model=8, hidden_size=16, num_layers=1 to address underfitting\n",
    "D_MODEL = 16         # Feature dimension from each branch (increased from 8→14→16 for better capacity)\n",
    "HIDDEN_SIZE = 32     # LSTM hidden size (increased from 16→28→32 for better capacity)\n",
    "NUM_LAYERS = 2       # Number of LSTM layers (increased from 1 to 2)\n",
    "DROPOUT = 0.82       # Dropout rate (balanced: between 0.8 and 0.85 for stable training)\n",
    "BIDIRECTIONAL = False # Use unidirectional LSTM (keep unidirectional)\n",
    "\n",
    "# Training hyperparameters with balanced regularization\n",
    "BATCH_SIZE = 32      # Smaller batches for more stable gradients\n",
    "INITIAL_LR = 5e-4    # Lower learning rate (reduced from 1e-3)\n",
    "WEIGHT_DECAY = 0.075 # Balanced L2 regularization (between 0.05 and 0.1 for stable training)\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "AUGMENTATION_PROB = 0.9  # Very aggressive augmentation (increased from 0.7)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LOUO: COMPACT LSTM MODEL CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Sequence length: {SEQUENCE_LENGTH} windows\")\n",
    "print(f\"  Window size: {WINDOW_SIZE} samples ({WINDOW_SIZE * 2}ms @ 500Hz)\")\n",
    "print(f\"  Stride: {STRIDE} samples ({STRIDE * 2}ms, 50% overlap)\")\n",
    "print(f\"  d_model: {D_MODEL} (increased from 8 to address underfitting)\")\n",
    "print(f\"  LSTM hidden size: {HIDDEN_SIZE} (increased from 16 to address underfitting)\")\n",
    "print(f\"  LSTM layers: {NUM_LAYERS} (increased from 1 to address underfitting)\")\n",
    "print(f\"  Dropout: {DROPOUT}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Learning rate: {INITIAL_LR}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Augmentation prob: {AUGMENTATION_PROB}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Set data directory - auto-detect Google Colab vs local\n",
    "current_dir = os.getcwd()\n",
    "is_colab = current_dir.startswith('/content')\n",
    "\n",
    "if is_colab:\n",
    "    # Google Colab path\n",
    "    DATA_DIR = '/content/drive/MyDrive/emg_dataset/dataset'\n",
    "    print(f\"Detected Google Colab environment\")\n",
    "else:\n",
    "    # Local path\n",
    "    DATA_DIR = '/Users/gabdomingo/Documents/emg_dataset/dataset'\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(f\"⚠️  Dataset directory not found: {DATA_DIR}\")\n",
    "    print(\"Please update DATA_DIR to point to your dataset folder\")\n",
    "    if is_colab:\n",
    "        print(\"\\nFor Google Colab:\")\n",
    "        print(\"  1. Make sure you've mounted Google Drive\")\n",
    "        print(\"  2. Check that the dataset is at: /content/drive/MyDrive/emg_dataset/dataset\")\n",
    "        print(\"  3. Verify the path contains session subfolders\")\n",
    "    raise FileNotFoundError(f\"Dataset directory not found: {DATA_DIR}\")\n",
    "\n",
    "print(f\"✓ Dataset directory found: {DATA_DIR}\")\n",
    "\n",
    "# Load full dataset to get session info\n",
    "full_dataset = EMGDataset(DATA_DIR, normalize=False, sequence_length=SEQUENCE_LENGTH,\n",
    "                         window_size=WINDOW_SIZE, stride=STRIDE)\n",
    "all_sessions = [s.name for s in full_dataset.sessions]\n",
    "\n",
    "# Group sessions by user\n",
    "def get_user_from_session(session_name):\n",
    "    \"\"\"Extract user name from session name\"\"\"\n",
    "    if 'user' in session_name:\n",
    "        user = session_name.split('user')[-1].split('/')[0]\n",
    "        return user\n",
    "    return None\n",
    "\n",
    "user_sessions = {}\n",
    "for session_name in all_sessions:\n",
    "    user = get_user_from_session(session_name)\n",
    "    if user:\n",
    "        if user not in user_sessions:\n",
    "            user_sessions[user] = []\n",
    "        user_sessions[user].append(session_name)\n",
    "\n",
    "all_users = sorted(user_sessions.keys())\n",
    "print(f\"Found {len(all_users)} users: {all_users}\")\n",
    "print(f\"Total sessions: {len(all_sessions)}\")\n",
    "\n",
    "# SINGLE USER TEST: Test on userB, train/validate on all other users\n",
    "# This avoids computationally expensive retraining on previously seen data\n",
    "# We train once, use validation to tune hyperparameters, then test on completely unseen userB\n",
    "test_user = 'B'  # Fixed test user: userB\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SINGLE USER TEST: Testing on user '{test_user}' (completely unseen)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"⚠️  Training once on all other users, validating to tune hyperparameters,\")\n",
    "print(f\"   then testing on user '{test_user}' (no retraining = no data leakage)\")\n",
    "\n",
    "# Results storage (single user, but keeping list format for compatibility)\n",
    "louo_results = []\n",
    "\n",
    "# Verify test user exists\n",
    "if test_user not in user_sessions:\n",
    "    raise ValueError(f\"❌ ERROR: Test user '{test_user}' not found in dataset. Available users: {all_users}\")\n",
    "\n",
    "# Get test sessions (all sessions from test user)\n",
    "test_sessions = user_sessions[test_user]\n",
    "\n",
    "# Get training sessions (all sessions from other users)\n",
    "train_val_sessions = []\n",
    "for user in all_users:\n",
    "    if user != test_user:\n",
    "        train_val_sessions.extend(user_sessions[user])\n",
    "\n",
    "print(f\"  Training/Validation sessions: {len(train_val_sessions)} (from {len(all_users)-1} users)\")\n",
    "print(f\"  Test sessions: {len(test_sessions)} (from user '{test_user}')\")\n",
    "\n",
    "# VALIDATION: Ensure no test user data leaks into training/validation\n",
    "# Fix: Check for exact user match (e.g., 'userA' should not match 'userA2')\n",
    "def session_belongs_to_user(session_name, user):\n",
    "    \"\"\"Check if session belongs to a specific user (exact match, not substring)\"\"\"\n",
    "    if 'user' not in session_name:\n",
    "        return False\n",
    "    # Extract user from session name\n",
    "    session_user = get_user_from_session(session_name)\n",
    "    return session_user == user\n",
    "\n",
    "# Check for data leakage\n",
    "test_user_in_train = any(session_belongs_to_user(s, test_user) for s in train_val_sessions)\n",
    "if test_user_in_train:\n",
    "    leaking_sessions = [s for s in train_val_sessions if session_belongs_to_user(s, test_user)]\n",
    "    raise ValueError(f\"❌ DATA LEAKAGE DETECTED: Test user '{test_user}' found in training/validation sessions!\\n\"\n",
    "                    f\"   Leaking sessions: {leaking_sessions}\")\n",
    "\n",
    "# Verify test sessions belong to test user\n",
    "for test_session in test_sessions:\n",
    "    if not session_belongs_to_user(test_session, test_user):\n",
    "        raise ValueError(f\"❌ ERROR: Test session '{test_session}' does not belong to test user '{test_user}'\")\n",
    "\n",
    "print(f\"  ✓ Data split validated: No leakage detected\")\n",
    "\n",
    "# Split training sessions into train/val (session-based within training users)\n",
    "# This split is used to tune hyperparameters via validation set\n",
    "np.random.seed(42)  # Reproducibility\n",
    "shuffled_train_val = train_val_sessions.copy()\n",
    "np.random.shuffle(shuffled_train_val)\n",
    "\n",
    "# 80% train, 20% val (within training users)\n",
    "split_idx = int(0.8 * len(shuffled_train_val))\n",
    "train_sessions = shuffled_train_val[:split_idx]\n",
    "validation_sessions = shuffled_train_val[split_idx:]\n",
    "\n",
    "print(f\"    Train: {len(train_sessions)} sessions\")\n",
    "print(f\"    Val: {len(validation_sessions)} sessions (for hyperparameter tuning)\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EMGDatasetFiltered(\n",
    "    DATA_DIR,\n",
    "    include_sessions=train_sessions,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    stride=STRIDE,\n",
    "    use_augmentation=True,\n",
    "    augmentation_prob=AUGMENTATION_PROB\n",
    ")\n",
    "\n",
    "validation_dataset = EMGDatasetFiltered(\n",
    "    DATA_DIR,\n",
    "    include_sessions=validation_sessions,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    stride=STRIDE,\n",
    "    scalers=train_dataset.scalers,\n",
    "    use_augmentation=False\n",
    ")\n",
    "\n",
    "test_dataset = EMGDatasetFiltered(\n",
    "    DATA_DIR,\n",
    "    include_sessions=test_sessions,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    stride=STRIDE,\n",
    "    scalers=train_dataset.scalers,\n",
    "    use_augmentation=False\n",
    ")\n",
    "\n",
    "# Calculate class weights\n",
    "train_labels = train_dataset.sequence_labels\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "class_weights = compute_class_weight('balanced', classes=unique, y=train_labels)\n",
    "class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
    "\n",
    "# Enhanced class imbalance handling - BALANCED weighting for minority class\n",
    "if len(unique) == 2:\n",
    "    minority_class_idx = np.argmin(counts)\n",
    "    majority_class_idx = np.argmax(counts)\n",
    "    class_ratio = min(counts) / max(counts)\n",
    "    \n",
    "    # Moderate boosting to address class bias without causing training instability\n",
    "    if class_ratio < 0.7:  # Back to original threshold for stability\n",
    "        # Moderate boost factor: max(2.5, 1.2 / class_ratio) - balanced approach\n",
    "        # This gives reasonable penalty for misclassifying minority class without destabilizing training\n",
    "        boost_factor = max(2.5, 1.2 / class_ratio)\n",
    "        original_weight = class_weights[minority_class_idx].item()\n",
    "        class_weights[minority_class_idx] *= boost_factor\n",
    "        minority_class_name = train_dataset.label_encoder.inverse_transform([minority_class_idx])[0]\n",
    "        print(f\"\\n  ⚡ BALANCED: Boosted {minority_class_name} class weight by {boost_factor:.2f}x\")\n",
    "        print(f\"     (Original weight: {original_weight:.4f}, New weight: {class_weights[minority_class_idx].item():.4f})\")\n",
    "    \n",
    "    class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "\n",
    "# Print diagnostics\n",
    "print(f\"\\n  Training class distribution:\")\n",
    "for cls, count in zip(unique, counts):\n",
    "    pct = 100 * count / len(train_labels)\n",
    "    class_name = train_dataset.label_encoder.inverse_transform([cls])[0]\n",
    "    print(f\"    {class_name}: {count} samples ({pct:.1f}%)\")\n",
    "\n",
    "# Check test distribution\n",
    "test_labels = test_dataset.sequence_labels\n",
    "unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "print(f\"\\n  Test user '{test_user}' class distribution:\")\n",
    "for cls, count in zip(unique_test, counts_test):\n",
    "    pct = 100 * count / len(test_labels) if len(test_labels) > 0 else 0\n",
    "    class_name = test_dataset.label_encoder.inverse_transform([cls])[0]\n",
    "    print(f\"    {class_name}: {count} samples ({pct:.1f}%)\")\n",
    "\n",
    "if len(unique_test) < 2:\n",
    "    print(f\"  ⚠️  WARNING: Only one class in test set for user '{test_user}'!\")\n",
    "\n",
    "# Create dataloaders\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights_sample = 1.0 / class_counts\n",
    "sample_weights = class_weights_sample[train_labels]\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=weighted_sampler, num_workers=0)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Verify model class signature before initialization\n",
    "import inspect\n",
    "model_signature = inspect.signature(EMGLSTMModel.__init__)\n",
    "expected_params = ['num_classes', 'd_model', 'hidden_size', 'num_layers', 'dropout', 'sequence_length', 'bidirectional']\n",
    "actual_params = list(model_signature.parameters.keys())[1:]  # Skip 'self'\n",
    "\n",
    "if 'num_classes' not in actual_params:\n",
    "    raise TypeError(\n",
    "        f\"EMGLSTMModel.__init__() is missing 'num_classes' parameter. \"\n",
    "        f\"Found parameters: {actual_params}\\n\"\n",
    "        f\"Please re-run the first cell (model definition) to reload the updated class.\"\n",
    "    )\n",
    "\n",
    "# Initialize model\n",
    "model = EMGLSTMModel(\n",
    "    num_classes=train_dataset.num_classes,\n",
    "    d_model=D_MODEL,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    bidirectional=BIDIRECTIONAL\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n  Model parameters: {total_params:,} (trainable: {trainable_params:,})\")\n",
    "print(f\"  Training samples: {len(train_dataset.sequences):,}\")\n",
    "print(f\"  Parameters per sample: {total_params / len(train_dataset.sequences):.2f}\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "# Balanced gamma: 2.2 (between 2.0 and 2.5) to focus on hard examples without over-emphasis\n",
    "criterion = FocalLoss(alpha=class_weights, gamma=2.2).to(DEVICE)\n",
    "print(f\"\\n  Loss function: Focal Loss with gamma=2.2 (balanced for hard example focus)\")\n",
    "print(f\"  Class weights: {dict(zip([train_dataset.label_encoder.inverse_transform([c])[0] for c in unique], class_weights.cpu().numpy()))}\")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.7, patience=5,\n",
    "    min_lr=1e-4, threshold=0.01\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "best_val_f1 = 0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\n  Starting training (validation set used for hyperparameter tuning)...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, DEVICE, print_per_class_loss=False\n",
    "    )\n",
    "    \n",
    "    # Validate (used for hyperparameter tuning: early stopping, LR scheduling)\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1, val_cm = evaluate(\n",
    "        model, validation_loader, criterion, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Update learning rate (hyperparameter tuning)\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Early stopping (hyperparameter tuning)\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            break\n",
    "    \n",
    "    # Print progress every 10 epochs with per-class validation metrics\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        # Calculate per-class validation metrics for diagnostics\n",
    "        with torch.no_grad():\n",
    "            all_val_preds = []\n",
    "            all_val_labels = []\n",
    "            for batch in validation_loader:\n",
    "                batch = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "                labels = batch['label']\n",
    "                logits = model(batch)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_val_preds.extend(preds.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            from sklearn.metrics import precision_recall_fscore_support\n",
    "            val_prec_per_class, val_rec_per_class, val_f1_per_class, _ = precision_recall_fscore_support(\n",
    "                all_val_labels, all_val_preds, average=None, zero_division=0\n",
    "            )\n",
    "            class_names = [train_dataset.label_encoder.inverse_transform([i])[0] for i in range(len(val_f1_per_class))]\n",
    "            \n",
    "            print(f\"    Epoch {epoch+1}: Train F1={train_f1:.4f}, Val F1={val_f1:.4f}\")\n",
    "            print(f\"      Val per-class F1: {dict(zip(class_names, val_f1_per_class))}\")\n",
    "            print(f\"      Val per-class Recall: {dict(zip(class_names, val_rec_per_class))}\")\n",
    "\n",
    "# Load best model (based on validation performance)\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# =============================================================\n",
    "# MODEL CONFIDENCE ANALYSIS (NO THRESHOLD TUNING)\n",
    "# =============================================================\n",
    "# Analyze model confidence on the validation set without changing predictions.\n",
    "# We will use argmax over softmax probabilities (equivalent to a 0.5 decision\n",
    "# boundary in the binary case) and keep thresholds out of the decision path.\n",
    "# =============================================================\n",
    "print(f\"\\n  Analyzing model confidence on validation set (argmax decision)...\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Get validation predictions (probabilities + argmax)\n",
    "val_probs = []\n",
    "val_labels_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        batch = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        labels = batch['label']\n",
    "        logits = model(batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        val_probs.extend(probs.cpu().numpy())\n",
    "        val_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "val_probs = np.array(val_probs)\n",
    "val_labels_list = np.array(val_labels_list)\n",
    "\n",
    "# Analyze model confidence\n",
    "val_max_probs = np.max(val_probs, axis=1)  # Maximum probability (confidence)\n",
    "val_confidence_mean = float(np.mean(val_max_probs))\n",
    "val_confidence_std = float(np.std(val_max_probs))\n",
    "val_confidence_median = float(np.median(val_max_probs))\n",
    "val_low_confidence_pct = float(100 * np.sum(val_max_probs < 0.60) / len(val_max_probs))\n",
    "val_very_low_confidence_pct = float(100 * np.sum(val_max_probs < 0.55) / len(val_max_probs))\n",
    "\n",
    "print(f\"    Model Confidence Statistics (validation):\")\n",
    "print(f\"      Mean confidence: {val_confidence_mean:.4f}\")\n",
    "print(f\"      Median confidence: {val_confidence_median:.4f}\")\n",
    "print(f\"      Std confidence: {val_confidence_std:.4f}\")\n",
    "print(f\"      Low confidence (<0.60): {val_low_confidence_pct:.1f}%\")\n",
    "print(f\"      Very low confidence (<0.55): {val_very_low_confidence_pct:.1f}%\")\n",
    "\n",
    "# Validation performance with argmax (no threshold tuning)\n",
    "val_preds = np.argmax(val_probs, axis=1)\n",
    "val_acc = accuracy_score(val_labels_list, val_preds)\n",
    "val_f1 = f1_score(val_labels_list, val_preds, average='weighted', zero_division=0)\n",
    "print(f\"\\n    Validation performance (argmax):\")\n",
    "print(f\"      Accuracy: {val_acc:.4f}\")\n",
    "print(f\"      F1 (weighted): {val_f1:.4f}\")\n",
    "\n",
    "# REST baseline confidence (Option B monitoring)\n",
    "# Assume REST is class index 0 (as in the dataset construction).\n",
    "rest_class_idx = 0\n",
    "rest_mask = val_labels_list == rest_class_idx\n",
    "if np.any(rest_mask):\n",
    "    rest_probs = val_probs[rest_mask, rest_class_idx]\n",
    "    val_rest_conf_mean = float(np.mean(rest_probs))\n",
    "    val_rest_conf_median = float(np.median(rest_probs))\n",
    "    val_rest_low_conf_pct = float(100 * np.sum(rest_probs < 0.60) / len(rest_probs))\n",
    "    print(f\"\\n    REST baseline (validation, true REST samples):\")\n",
    "    print(f\"      Mean REST confidence: {val_rest_conf_mean:.4f}\")\n",
    "    print(f\"      Median REST confidence: {val_rest_conf_median:.4f}\")\n",
    "    print(f\"      Low REST confidence (<0.60): {val_rest_low_conf_pct:.1f}%\")\n",
    "else:\n",
    "    val_rest_conf_mean = 0.0\n",
    "    val_rest_conf_median = 0.0\n",
    "    val_rest_low_conf_pct = 0.0\n",
    "    print(f\"\\n    REST baseline (validation): no REST samples found.\")\n",
    "\n",
    "# For compatibility with export code, define best_threshold as the nominal 0.50\n",
    "# decision boundary (but DO NOT use it to override argmax decisions).\n",
    "best_threshold = 0.50\n",
    "best_f1 = val_f1\n",
    "best_rest_recall = None\n",
    "best_fist_recall = None\n",
    "\n",
    "print(f\"\\n    Note: No threshold tuning performed. Decisions use argmax over softmax.\")\n",
    "\n",
    "# =============================================================\n",
    "# TEST EVALUATION (ARGMAX DECISION RULE)\n",
    "# =============================================================\n",
    "print(f\"\\n  Evaluating on test set (argmax decision)...\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Evaluate with the standard evaluate() helper\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1, test_cm = evaluate(\n",
    "    model, test_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "# Compute per-class metrics on test set\n",
    "model.eval()\n",
    "test_probs = []\n",
    "test_labels_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        labels = batch['label']\n",
    "        logits = model(batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "test_probs = np.array(test_probs)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "test_preds = np.argmax(test_probs, axis=1)\n",
    "prec_per_class, rec_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds, average=None, zero_division=0\n",
    ")\n",
    "class_names = [test_dataset.label_encoder.inverse_transform([i])[0] for i in range(len(f1_per_class))]\n",
    "\n",
    "# Test confidence statistics\n",
    "test_max_probs = np.max(test_probs, axis=1)\n",
    "test_confidence_mean = float(np.mean(test_max_probs))\n",
    "test_confidence_median = float(np.median(test_max_probs))\n",
    "test_low_confidence_pct = float(100 * np.sum(test_max_probs < 0.60) / len(test_max_probs))\n",
    "\n",
    "print(f\"    Test set confidence statistics:\")\n",
    "print(f\"      Mean confidence: {test_confidence_mean:.4f}\")\n",
    "print(f\"      Median confidence: {test_confidence_median:.4f}\")\n",
    "print(f\"      Low confidence (<0.60): {test_low_confidence_pct:.1f}%\")\n",
    "\n",
    "# REST baseline confidence on test set (for monitoring reference)\n",
    "rest_mask_test = test_labels_list == rest_class_idx\n",
    "if np.any(rest_mask_test):\n",
    "    rest_probs_test = test_probs[rest_mask_test, rest_class_idx]\n",
    "    test_rest_conf_mean = float(np.mean(rest_probs_test))\n",
    "    test_rest_low_conf_pct = float(100 * np.sum(rest_probs_test < 0.60) / len(rest_probs_test))\n",
    "else:\n",
    "    test_rest_conf_mean = 0.0\n",
    "    test_rest_low_conf_pct = 0.0\n",
    "\n",
    "print(f\"\\n  ✓ User '{test_user}' Results (ARGMAX DECISION):\")\n",
    "print(f\"    Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"    Test F1 (weighted): {test_f1:.4f}\")\n",
    "print(f\"    Test Precision (weighted): {test_prec:.4f}\")\n",
    "print(f\"    Test Recall (weighted): {test_rec:.4f}\")\n",
    "\n",
    "print(f\"\\n    Per-Class Test Performance:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"      {class_name}:\")\n",
    "    print(f\"        Precision: {prec_per_class[i]:.4f}\")\n",
    "    print(f\"        Recall: {rec_per_class[i]:.4f}\")\n",
    "    print(f\"        F1: {f1_per_class[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n    Confusion Matrix (argmax):\")\n",
    "print(f\"      {test_cm}\")\n",
    "print(f\"      (Rows = True labels, Columns = Predicted labels)\")\n",
    "\n",
    "print(f\"\\n    Safety assessment (REST vs FIST):\")\n",
    "if len(prec_per_class) >= 2:\n",
    "    print(f\"      REST recall: {rec_per_class[0]:.4f} ({rec_per_class[0]*100:.1f}% of REST gestures detected)\")\n",
    "    print(f\"      REST precision: {prec_per_class[0]:.4f} ({prec_per_class[0]*100:.1f}% precision - false stops rate)\")\n",
    "    print(f\"      FIST recall: {rec_per_class[1]:.4f} ({rec_per_class[1]*100:.1f}% of FIST gestures detected)\")\n",
    "\n",
    "# Store results (single-user test)\n",
    "louo_results.append({\n",
    "    'test_user': test_user,\n",
    "    'train_samples': len(train_dataset.sequences),\n",
    "    'val_samples': len(validation_dataset.sequences),\n",
    "    'test_samples': len(test_dataset.sequences),\n",
    "    'best_val_f1': best_val_f1,\n",
    "    'optimal_threshold': float(best_threshold),  # nominal, not tuned\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_precision': float(test_prec),\n",
    "    'test_recall': float(test_rec),\n",
    "    'test_f1': float(test_f1),\n",
    "    'test_loss': float(test_loss),\n",
    "    'confusion_matrix': test_cm.tolist(),\n",
    "    'test_class_distribution': dict(zip(\n",
    "        [test_dataset.label_encoder.inverse_transform([c])[0] for c in unique_test],\n",
    "        counts_test.tolist()\n",
    "    )),\n",
    "    'deployment_approach': 'argmax',\n",
    "    'deployment_threshold': float(best_threshold)\n",
    "})\n",
    "\n",
    "# Per-class metrics for export\n",
    "test_prec_per_class = prec_per_class\n",
    "test_rec_per_class = rec_per_class\n",
    "\n",
    "# SINGLE USER TEST RESULTS SUMMARY (unchanged structure)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SINGLE USER TEST RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if len(louo_results) == 1:\n",
    "    r = louo_results[0]\n",
    "    print(f\"\\nTest User: {r['test_user']}\")\n",
    "    print(f\"  Test Samples: {r['test_samples']}\")\n",
    "    print(f\"  Training Samples: {r['train_samples']}\")\n",
    "    print(f\"  Validation Samples: {r['val_samples']}\")\n",
    "    print(f\"  Best Validation F1: {r['best_val_f1']:.4f}\")\n",
    "    print(f\"  Nominal Threshold (unused for decisions): {r.get('optimal_threshold', 0.5):.2f}\")\n",
    "    print(f\"\\n  Test Performance (argmax):\")\n",
    "    print(f\"    Accuracy: {r['test_accuracy']:.4f}\")\n",
    "    print(f\"    F1 Score: {r['test_f1']:.4f}\")\n",
    "    print(f\"    Precision: {r['test_precision']:.4f}\")\n",
    "    print(f\"    Recall: {r['test_recall']:.4f}\")\n",
    "    print(f\"    Confusion Matrix:\")\n",
    "    print(f\"      {np.array(r['confusion_matrix'])}\")\n",
    "    \n",
    "    avg_test_acc = r['test_accuracy']\n",
    "    avg_test_f1 = r['test_f1']\n",
    "    avg_test_prec = r['test_precision']\n",
    "    avg_test_rec = r['test_recall']\n",
    "    std_test_f1 = 0.0\n",
    "else:\n",
    "    avg_test_acc = np.mean([r['test_accuracy'] for r in louo_results])\n",
    "    avg_test_f1 = np.mean([r['test_f1'] for r in louo_results])\n",
    "    avg_test_prec = np.mean([r['test_precision'] for r in louo_results])\n",
    "    avg_test_rec = np.mean([r['test_recall'] for r in louo_results])\n",
    "    std_test_f1 = np.std([r['test_f1'] for r in louo_results]) if len(louo_results) > 1 else 0.0\n",
    "test_probs = []\n",
    "test_labels_list = []\n",
    "test_logits_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        labels = batch['label']\n",
    "        logits = model(batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "        test_logits_list.extend(logits.cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "test_probs = np.array(test_probs)\n",
    "test_logits = np.array(test_logits_list)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "# Analyze test set confidence\n",
    "test_max_probs = np.max(test_probs, axis=1)\n",
    "test_confidence_mean = np.mean(test_max_probs)\n",
    "test_confidence_median = np.median(test_max_probs)\n",
    "test_low_confidence_pct = 100 * np.sum(test_max_probs < 0.60) / len(test_max_probs)\n",
    "\n",
    "print(f\"    Test set confidence statistics:\")\n",
    "print(f\"      Mean confidence: {test_confidence_mean:.4f}\")\n",
    "print(f\"      Median confidence: {test_confidence_median:.4f}\")\n",
    "print(f\"      Low confidence (<0.60): {test_low_confidence_pct:.1f}%\")\n",
    "\n",
    "if test_confidence_mean < 0.65:\n",
    "    print(f\"\\n    ⚠️  WARNING: Test set confidence is LOW (mean={test_confidence_mean:.4f} < 0.65)\")\n",
    "    print(f\"       Model is uncertain on test data. This confirms underfitting concerns.\")\n",
    "    print(f\"       Threshold adjustment ({best_threshold:.2f}) is masking this uncertainty.\")\n",
    "elif test_confidence_mean < 0.75:\n",
    "    print(f\"\\n    ⚠️  CAUTION: Test set confidence is MODERATE (mean={test_confidence_mean:.4f} < 0.75)\")\n",
    "    print(f\"       Model confidence could be improved, but threshold adjustment helps.\")\n",
    "else:\n",
    "    print(f\"\\n    ✓ Test set confidence is GOOD (mean={test_confidence_mean:.4f} ≥ 0.75)\")\n",
    "    print(f\"       Model is producing confident predictions on unseen data.\")\n",
    "\n",
    "# Apply optimal threshold (baseline)\n",
    "test_preds_thresh = (test_probs[:, 1] > best_threshold).astype(int)\n",
    "\n",
    "# =============================================================\n",
    "# HYBRID SAFETY APPROACH: Confidence-based fallback + Gesture confirmation\n",
    "# =============================================================\n",
    "# For safety-critical wheelchair control:\n",
    "# 1. Use threshold 0.70 for normal operation\n",
    "# 2. If confidence is low (<0.60), default to REST (stop) - fail-safe\n",
    "# 3. Require REST gesture for 2-3 consecutive windows before stopping\n",
    "#    This prevents false stops from single misclassified windows\n",
    "# =============================================================\n",
    "\n",
    "def hybrid_safety_predictions(probs, threshold, confidence_threshold=0.60, rest_confirmation_windows=2):\n",
    "    \"\"\"\n",
    "    Hybrid safety approach for wheelchair control:\n",
    "    - Normal operation: Use threshold for predictions\n",
    "    - Low confidence: Default to REST (stop) - fail-safe\n",
    "    - REST confirmation: Require N consecutive REST predictions before confirming stop\n",
    "    \n",
    "    Args:\n",
    "        probs: (N, 2) array of class probabilities\n",
    "        threshold: Decision threshold for FIST (class 1)\n",
    "        confidence_threshold: Minimum confidence to trust prediction (default: 0.60)\n",
    "        rest_confirmation_windows: Number of consecutive REST predictions needed (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: (N,) array of final predictions with safety mechanisms\n",
    "        confidence_scores: (N,) array of confidence scores (max probability)\n",
    "    \"\"\"\n",
    "    n_samples = len(probs)\n",
    "    predictions = np.zeros(n_samples, dtype=int)\n",
    "    confidence_scores = np.max(probs, axis=1)\n",
    "    \n",
    "    # Step 1: Initial predictions with confidence-based fallback\n",
    "    initial_preds = np.zeros(n_samples, dtype=int)\n",
    "    for i in range(n_samples):\n",
    "        max_prob = confidence_scores[i]\n",
    "        if max_prob < confidence_threshold:\n",
    "            # Low confidence - default to REST (stop) for safety\n",
    "            initial_preds[i] = 0  # REST\n",
    "        else:\n",
    "            # Normal threshold-based prediction\n",
    "            initial_preds[i] = 1 if probs[i, 1] > threshold else 0\n",
    "    \n",
    "    # Step 2: Gesture confirmation for REST (stop) - require consecutive REST predictions\n",
    "    # This prevents false stops from single misclassified windows\n",
    "    rest_confirmation_count = 0\n",
    "    for i in range(n_samples):\n",
    "        if initial_preds[i] == 0:  # REST predicted\n",
    "            rest_confirmation_count += 1\n",
    "            if rest_confirmation_count >= rest_confirmation_windows:\n",
    "                # Confirmed REST - safe to stop\n",
    "                predictions[i] = 0  # REST\n",
    "            else:\n",
    "                # Not enough consecutive REST - keep previous state (FIST/continue)\n",
    "                # For first window, default to FIST if not confirmed\n",
    "                if i == 0:\n",
    "                    predictions[i] = 1  # FIST (continue)\n",
    "                else:\n",
    "                    predictions[i] = predictions[i-1]  # Maintain previous state\n",
    "        else:  # FIST predicted\n",
    "            rest_confirmation_count = 0  # Reset counter\n",
    "            predictions[i] = 1  # FIST (continue)\n",
    "    \n",
    "    return predictions, confidence_scores\n",
    "\n",
    "# Apply hybrid safety approach\n",
    "print(f\"\\n  Applying HYBRID SAFETY MECHANISM:\")\n",
    "print(f\"    - Confidence threshold: 0.60 (low confidence → default to REST/stop)\")\n",
    "print(f\"    - REST confirmation: 2 consecutive windows required\")\n",
    "test_preds_hybrid, test_confidence = hybrid_safety_predictions(\n",
    "    test_probs, \n",
    "    threshold=best_threshold,\n",
    "    confidence_threshold=0.60,\n",
    "    rest_confirmation_windows=2\n",
    ")\n",
    "\n",
    "# Calculate metrics for all three approaches\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# 1. Default threshold (argmax)\n",
    "test_preds_default = np.argmax(test_probs, axis=1)\n",
    "test_acc_default = accuracy_score(test_labels_list, test_preds_default)\n",
    "test_prec_default, test_rec_default, test_f1_default, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds_default, average='weighted', zero_division=0\n",
    ")\n",
    "test_cm_default = confusion_matrix(test_labels_list, test_preds_default)\n",
    "\n",
    "# 2. Optimal threshold (baseline)\n",
    "test_acc_thresh = accuracy_score(test_labels_list, test_preds_thresh)\n",
    "test_prec_thresh, test_rec_thresh, test_f1_thresh, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds_thresh, average='weighted', zero_division=0\n",
    ")\n",
    "test_cm_thresh = confusion_matrix(test_labels_list, test_preds_thresh)\n",
    "\n",
    "# 3. Hybrid safety approach (with confidence fallback + gesture confirmation)\n",
    "test_acc_hybrid = accuracy_score(test_labels_list, test_preds_hybrid)\n",
    "test_prec_hybrid, test_rec_hybrid, test_f1_hybrid, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds_hybrid, average='weighted', zero_division=0\n",
    ")\n",
    "test_cm_hybrid = confusion_matrix(test_labels_list, test_preds_hybrid)\n",
    "\n",
    "# Use OPTIMAL THRESHOLD metrics for final results (best performance)\n",
    "# The hybrid approach was tested but optimal threshold (0.70) performs better\n",
    "test_acc = test_acc_thresh\n",
    "test_prec = test_prec_thresh\n",
    "test_rec = test_rec_thresh\n",
    "test_f1 = test_f1_thresh\n",
    "test_cm = test_cm_thresh\n",
    "test_loss = 0.0  # Loss not meaningful with threshold adjustment\n",
    "\n",
    "# Calculate per-class metrics for all three approaches\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "prec_default, rec_default, f1_default, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds_default, average=None, zero_division=0\n",
    ")\n",
    "prec_thresh, rec_thresh, f1_thresh_class, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds_thresh, average=None, zero_division=0\n",
    ")\n",
    "prec_hybrid, rec_hybrid, f1_hybrid_class, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds_hybrid, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "# Calculate confidence statistics\n",
    "low_confidence_count = np.sum(test_confidence < 0.60)\n",
    "low_confidence_pct = 100 * low_confidence_count / len(test_confidence)\n",
    "\n",
    "print(f\"    Comparison of all approaches:\")\n",
    "print(f\"      Default (argmax, threshold=0.50):\")\n",
    "print(f\"        Overall: Acc={test_acc_default:.4f}, F1={test_f1_default:.4f}\")\n",
    "print(f\"        REST:    Prec={prec_default[0]:.4f}, Rec={rec_default[0]:.4f}, F1={f1_default[0]:.4f}\")\n",
    "print(f\"        FIST:    Prec={prec_default[1]:.4f}, Rec={rec_default[1]:.4f}, F1={f1_default[1]:.4f}\")\n",
    "print(f\"      Optimal threshold (threshold={best_threshold:.2f}):\")\n",
    "print(f\"        Overall: Acc={test_acc_thresh:.4f}, F1={test_f1_thresh:.4f}\")\n",
    "print(f\"        REST:    Prec={prec_thresh[0]:.4f}, Rec={rec_thresh[0]:.4f}, F1={f1_thresh_class[0]:.4f}\")\n",
    "print(f\"        FIST:    Prec={prec_thresh[1]:.4f}, Rec={rec_thresh[1]:.4f}, F1={f1_thresh_class[1]:.4f}\")\n",
    "print(f\"      HYBRID SAFETY (threshold={best_threshold:.2f} + confidence fallback + confirmation):\")\n",
    "print(f\"        Overall: Acc={test_acc_hybrid:.4f}, F1={test_f1_hybrid:.4f}\")\n",
    "print(f\"        REST:    Prec={prec_hybrid[0]:.4f}, Rec={rec_hybrid[0]:.4f}, F1={f1_hybrid_class[0]:.4f}\")\n",
    "print(f\"        FIST:    Prec={prec_hybrid[1]:.4f}, Rec={rec_hybrid[1]:.4f}, F1={f1_hybrid_class[1]:.4f}\")\n",
    "print(f\"        Low confidence samples: {low_confidence_count}/{len(test_confidence)} ({low_confidence_pct:.1f}%) → defaulted to REST\")\n",
    "print(f\"\\n    Comparison: Optimal threshold vs Default:\")\n",
    "print(f\"      REST recall: {rec_default[0]:.4f} → {rec_thresh[0]:.4f} ({((rec_thresh[0]-rec_default[0])/rec_default[0]*100):+.1f}%)\")\n",
    "print(f\"      Overall F1:   {test_f1_default:.4f} → {test_f1_thresh:.4f} ({((test_f1_thresh-test_f1_default)/test_f1_default*100):+.1f}%)\")\n",
    "print(f\"\\n    Note: Optimal threshold (0.70) selected for deployment - provides best balance\")\n",
    "print(f\"          of REST recall (safety) and overall performance.\")\n",
    "\n",
    "# Store results\n",
    "louo_results.append({\n",
    "    'test_user': test_user,\n",
    "    'train_samples': len(train_dataset.sequences),\n",
    "    'val_samples': len(validation_dataset.sequences),\n",
    "    'test_samples': len(test_dataset.sequences),\n",
    "    'best_val_f1': best_val_f1,\n",
    "    'optimal_threshold': float(best_threshold),\n",
    "    'test_accuracy': test_acc,  # Using optimal threshold approach\n",
    "    'test_precision': test_prec,\n",
    "    'test_recall': test_rec,\n",
    "    'test_f1': test_f1,\n",
    "    'test_loss': test_loss,\n",
    "    'confusion_matrix': test_cm.tolist(),  # Using optimal threshold approach\n",
    "    'test_class_distribution': dict(zip(\n",
    "        [test_dataset.label_encoder.inverse_transform([c])[0] for c in unique_test],\n",
    "        counts_test.tolist()\n",
    "    )),\n",
    "    # Store all approach metrics for comparison\n",
    "    'test_accuracy_default': float(test_acc_default),\n",
    "    'test_f1_default': float(test_f1_default),\n",
    "    'confusion_matrix_default': test_cm_default.tolist(),\n",
    "    'test_accuracy_thresh': float(test_acc_thresh),\n",
    "    'test_f1_thresh': float(test_f1_thresh),\n",
    "    'confusion_matrix_thresh': test_cm_thresh.tolist(),\n",
    "    'test_accuracy_hybrid': float(test_acc_hybrid),\n",
    "    'test_f1_hybrid': float(test_f1_hybrid),\n",
    "    'confusion_matrix_hybrid': test_cm_hybrid.tolist(),\n",
    "    'hybrid_safety_config': {\n",
    "        'confidence_threshold': 0.60,\n",
    "        'rest_confirmation_windows': 2,\n",
    "        'low_confidence_samples': int(low_confidence_count),\n",
    "        'low_confidence_percentage': float(low_confidence_pct),\n",
    "        'note': 'Hybrid approach tested but optimal threshold (0.70) performs better - using optimal threshold for deployment'\n",
    "    },\n",
    "    'deployment_approach': 'optimal_threshold',  # Using optimal threshold, not hybrid\n",
    "    'deployment_threshold': float(best_threshold)\n",
    "})\n",
    "\n",
    "# Calculate per-class test metrics for detailed analysis (using threshold-adjusted predictions)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Use threshold-adjusted predictions (already computed above)\n",
    "test_prec_per_class, test_rec_per_class, test_f1_per_class, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds_thresh, average=None, zero_division=0\n",
    ")\n",
    "class_names = [test_dataset.label_encoder.inverse_transform([i])[0] for i in range(len(test_f1_per_class))]\n",
    "\n",
    "# Calculate per-class metrics for optimal threshold approach (final results)\n",
    "test_prec_per_class, test_rec_per_class, test_f1_per_class, _ = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds_thresh, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\n  ✓ User '{test_user}' Results (OPTIMAL THRESHOLD APPROACH):\")\n",
    "print(f\"    Configuration:\")\n",
    "print(f\"      - Threshold: {best_threshold:.2f} (safety-first selection)\")\n",
    "print(f\"      - Selection priority: REST recall > FIST recall > Overall F1\")\n",
    "print(f\"      - Validation REST recall: {best_rest_recall:.4f}\")\n",
    "print(f\"\\n    Test Performance:\")\n",
    "print(f\"      Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"      Test F1 (weighted): {test_f1:.4f}\")\n",
    "print(f\"      Test Precision (weighted): {test_prec:.4f}\")\n",
    "print(f\"      Test Recall (weighted): {test_rec:.4f}\")\n",
    "print(f\"\\n    Per-Class Test Performance:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"      {class_name}:\")\n",
    "    print(f\"        Precision: {test_prec_per_class[i]:.4f}\")\n",
    "    print(f\"        Recall: {test_rec_per_class[i]:.4f}\")\n",
    "    print(f\"        F1: {test_f1_per_class[i]:.4f}\")\n",
    "print(f\"\\n    Confusion Matrix (threshold={best_threshold:.2f}):\")\n",
    "print(f\"      {test_cm}\")\n",
    "print(f\"      (Rows = True labels, Columns = Predicted labels)\")\n",
    "print(f\"\\n    Safety assessment:\")\n",
    "print(f\"      - REST recall: {test_rec_per_class[0]:.4f} ({test_rec_per_class[0]*100:.1f}% of REST gestures detected)\")\n",
    "print(f\"      - REST precision: {test_prec_per_class[0]:.4f} ({test_prec_per_class[0]*100:.1f}% precision - very few false stops)\")\n",
    "print(f\"      - FIST recall: {test_rec_per_class[1]:.4f} ({test_rec_per_class[1]*100:.1f}% of FIST gestures detected)\")\n",
    "print(f\"      - Note: Threshold selected to maximize REST recall for safety-critical wheelchair control\")\n",
    "\n",
    "# =============================================================\n",
    "# SINGLE USER TEST RESULTS SUMMARY\n",
    "# =============================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SINGLE USER TEST RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Since we're testing on a single user, just show that user's results\n",
    "if len(louo_results) == 1:\n",
    "    r = louo_results[0]\n",
    "    print(f\"\\nTest User: {r['test_user']}\")\n",
    "    print(f\"  Test Samples: {r['test_samples']}\")\n",
    "    print(f\"  Training Samples: {r['train_samples']}\")\n",
    "    print(f\"  Validation Samples: {r['val_samples']}\")\n",
    "    print(f\"  Best Validation F1: {r['best_val_f1']:.4f}\")\n",
    "    print(f\"  Optimal Threshold: {r.get('optimal_threshold', 0.5):.2f}\")\n",
    "    print(f\"\\n  Test Performance (with threshold={r.get('optimal_threshold', 0.5):.2f}):\")\n",
    "    print(f\"    Accuracy: {r['test_accuracy']:.4f}\")\n",
    "    print(f\"    F1 Score: {r['test_f1']:.4f}\")\n",
    "    print(f\"    Precision: {r['test_precision']:.4f}\")\n",
    "    print(f\"    Recall: {r['test_recall']:.4f}\")\n",
    "    print(f\"    Confusion Matrix:\")\n",
    "    print(f\"      {np.array(r['confusion_matrix'])}\")\n",
    "    if 'test_f1_default' in r:\n",
    "        print(f\"\\n  Comparison (default threshold=0.50):\")\n",
    "        print(f\"    Accuracy: {r['test_accuracy_default']:.4f}\")\n",
    "        print(f\"    F1 Score: {r['test_f1_default']:.4f}\")\n",
    "    \n",
    "    # For compatibility with summary code, set these values\n",
    "    avg_test_acc = r['test_accuracy']\n",
    "    avg_test_f1 = r['test_f1']\n",
    "    avg_test_prec = r['test_precision']\n",
    "    avg_test_rec = r['test_recall']\n",
    "    std_test_f1 = 0.0  # Single result, no std dev\n",
    "else:\n",
    "    # Fallback if somehow multiple results exist\n",
    "    avg_test_acc = np.mean([r['test_accuracy'] for r in louo_results])\n",
    "    avg_test_f1 = np.mean([r['test_f1'] for r in louo_results])\n",
    "    avg_test_prec = np.mean([r['test_precision'] for r in louo_results])\n",
    "    avg_test_rec = np.mean([r['test_recall'] for r in louo_results])\n",
    "    std_test_f1 = np.std([r['test_f1'] for r in louo_results]) if len(louo_results) > 1 else 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b07a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# MODEL EXPORT FOR DEPLOYMENT\n",
    "# =============================================================\n",
    "# Export the trained model, threshold, and configuration for Raspberry Pi deployment\n",
    "# =============================================================\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Create deployment directory\n",
    "deployment_dir = Path('deployment')\n",
    "deployment_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXPORTING MODEL FOR DEPLOYMENT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Verify model configuration matches training\n",
    "print(f\"\\n  Verifying model configuration:\")\n",
    "print(f\"    d_model: {D_MODEL}\")\n",
    "print(f\"    hidden_size: {HIDDEN_SIZE}\")\n",
    "print(f\"    num_layers: {NUM_LAYERS}\")\n",
    "print(f\"    dropout: {DROPOUT}\")\n",
    "print(f\"    sequence_length: {SEQUENCE_LENGTH}\")\n",
    "print(f\"    bidirectional: {BIDIRECTIONAL}\")\n",
    "print(f\"    Total parameters: {total_params:,}\")\n",
    "\n",
    "# Verify StandardScalers exist\n",
    "required_scalers = ['raw', 'rms_lms', 'wiener_td', 'wiener_fft', 'imu', 'spectral_raw', 'spectral_wiener']\n",
    "missing_scalers = [s for s in required_scalers if s not in train_dataset.scalers or train_dataset.scalers[s] is None]\n",
    "\n",
    "if missing_scalers:\n",
    "    raise ValueError(f\"❌ Missing StandardScalers: {missing_scalers}. Cannot deploy without complete normalization.\")\n",
    "else:\n",
    "    print(f\"\\n  ✓ All StandardScalers present: {required_scalers}\")\n",
    "\n",
    "# 1. Save model state dict\n",
    "model_path = deployment_dir / 'emg_lstm_model.pt'\n",
    "model_config = {\n",
    "    'num_classes': train_dataset.num_classes,\n",
    "    'd_model': D_MODEL,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'dropout': DROPOUT,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'bidirectional': BIDIRECTIONAL\n",
    "}\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'class_names': train_dataset.label_encoder.classes_.tolist(),\n",
    "    'label_encoder': train_dataset.label_encoder\n",
    "}, model_path)\n",
    "print(f\"\\n✓ Model saved to: {model_path}\")\n",
    "print(f\"  Model config: {model_config}\")\n",
    "\n",
    "# 2. Save deployment configuration\n",
    "deployment_config = {\n",
    "    'optimal_threshold': float(best_threshold),  # nominal 0.50, not used to gate decisions\n",
    "    'window_size': WINDOW_SIZE,\n",
    "    'stride': STRIDE,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'sampling_rate': 500,  # EMG sampling rate\n",
    "    'imu_sampling_rate': 100,  # IMU sampling rate\n",
    "    'normalization_method': 'two-step',  # Per-window z-score + global StandardScaler\n",
    "    'scaler_file': 'standard_scalers.pkl',  # StandardScalers for deployment\n",
    "    'model_parameters': int(total_params),\n",
    "    'performance_metrics': {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_f1': float(test_f1),\n",
    "        'test_precision': float(test_prec),\n",
    "        'test_recall': float(test_rec),\n",
    "        'rest_recall': float(test_rec_per_class[0]),\n",
    "        'rest_precision': float(test_prec_per_class[0]),\n",
    "        'fist_recall': float(test_rec_per_class[1]),\n",
    "        'fist_precision': float(test_prec_per_class[1])\n",
    "    },\n",
    "    'confidence_metrics': {\n",
    "        'validation_confidence_mean': float(val_confidence_mean),\n",
    "        'validation_confidence_median': float(val_confidence_median),\n",
    "        'validation_low_confidence_pct': float(val_low_confidence_pct),\n",
    "        'test_confidence_mean': float(test_confidence_mean),\n",
    "        'test_confidence_median': float(test_confidence_median),\n",
    "        'test_low_confidence_pct': float(test_low_confidence_pct),\n",
    "        'confidence_warning': 'LOW' if test_confidence_mean < 0.65 else ('MODERATE' if test_confidence_mean < 0.75 else 'GOOD')\n",
    "    },\n",
    "    'rest_baseline': {\n",
    "        'validation_mean_confidence': float(val_rest_conf_mean),\n",
    "        'validation_low_confidence_pct': float(val_rest_low_conf_pct),\n",
    "        'test_mean_confidence': float(test_rest_conf_mean),\n",
    "        'test_low_confidence_pct': float(test_rest_low_conf_pct),\n",
    "        'note': 'For monitoring only; early-session REST can be compared to these values on-device.'\n",
    "    },\n",
    "    'deployment_date': str(pd.Timestamp.now()),\n",
    "    'test_user': test_user,\n",
    "    'notes': 'Model trained with per-window + global normalization; deployment uses argmax over softmax (no tuned threshold).',\n",
    "    'threshold_warning': 'Controller must not gate decisions on probability thresholds; use argmax and optional temporal smoothing only.'\n",
    "}\n",
    "\n",
    "config_path = deployment_dir / 'deployment_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "print(f\"✓ Configuration saved to: {config_path}\")\n",
    "\n",
    "# 2b. Save StandardScalers for deployment normalization (STEP 2 of two-step normalization)\n",
    "scaler_path = deployment_dir / 'standard_scalers.pkl'\n",
    "scaler_data = {\n",
    "    'raw': train_dataset.scalers.get('raw'),\n",
    "    'rms_lms': train_dataset.scalers.get('rms_lms'),\n",
    "    'wiener_td': train_dataset.scalers.get('wiener_td'),\n",
    "    'wiener_fft': train_dataset.scalers.get('wiener_fft'),\n",
    "    'imu': train_dataset.scalers.get('imu'),\n",
    "    'spectral_raw': train_dataset.scalers.get('spectral_raw'),\n",
    "    'spectral_wiener': train_dataset.scalers.get('spectral_wiener')\n",
    "}\n",
    "\n",
    "# Verify all scalers are present and not None\n",
    "for key, scaler in scaler_data.items():\n",
    "    if scaler is None:\n",
    "        raise ValueError(f\"❌ StandardScaler '{key}' is None. Cannot deploy without complete normalization.\")\n",
    "\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler_data, f)\n",
    "print(f\"\\n✓ StandardScalers saved to: {scaler_path}\")\n",
    "print(f\"  Scaler keys: {list(scaler_data.keys())}\")\n",
    "print(f\"  Note: These scalers are required for STEP 2 of two-step normalization in deployment\")\n",
    "\n",
    "\n",
    "# 3. Create model summary\n",
    "summary = {\n",
    "    'model_file': str(model_path),\n",
    "    'config_file': str(config_path),\n",
    "    'model_size_mb': total_params * 4 / (1024 * 1024),  # Assuming float32 (4 bytes)\n",
    "    'threshold': float(best_threshold),\n",
    "    'performance': deployment_config['performance_metrics']\n",
    "}\n",
    "\n",
    "summary_path = deployment_dir / 'model_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"✓ Model summary saved to: {summary_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DEPLOYMENT PACKAGE READY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Deployment directory: {deployment_dir.absolute()}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  1. {model_path.name} - Trained model weights (d_model={D_MODEL}, hidden_size={HIDDEN_SIZE}, layers={NUM_LAYERS})\")\n",
    "print(f\"  2. {config_path.name} - Deployment configuration (threshold={best_threshold:.2f})\")\n",
    "print(f\"  3. {scaler_path.name} - StandardScalers for two-step normalization (STEP 2)\")\n",
    "print(f\"  4. {summary_path.name} - Model summary\")\n",
    "print(f\"\\nDeployment package verification:\")\n",
    "print(f\"  ✓ Model architecture: {model_config}\")\n",
    "print(f\"  ✓ StandardScalers: {len(scaler_data)} scalers saved\")\n",
    "print(f\"  ✓ Optimal threshold: {best_threshold:.2f}\")\n",
    "print(f\"  ✓ Normalization: Two-step (per-window z-score + global StandardScaler)\")\n",
    "print(f\"  ✓ Model parameters: {total_params:,} ({total_params * 4 / (1024 * 1024):.2f} MB)\")\n",
    "print(f\"\\n⚠️  IMPORTANT: Update inference.py to load and apply StandardScalers!\")\n",
    "print(f\"   The deployment code must apply both normalization steps:\")\n",
    "print(f\"   STEP 1: Per-window z-score normalization (already implemented)\")\n",
    "print(f\"   STEP 2: Global StandardScaler normalization (MUST be added to inference.py)\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Update inference.py to load and apply standard_scalers.pkl\")\n",
    "print(f\"  2. Copy 'deployment' folder to Raspberry Pi\")\n",
    "print(f\"  3. On Raspberry Pi: pip install -r requirements.txt\")\n",
    "print(f\"  4. Run inference: python inference.py --help\")\n",
    "print(f\"  5. Model is ready for edge deployment! 🚀\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd41101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
